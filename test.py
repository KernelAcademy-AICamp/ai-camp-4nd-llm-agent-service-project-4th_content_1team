"""
Reddit 403 ìš°íšŒ - ìì—°ìŠ¤ëŸ¬ìš´ ì ‘ê·¼ ë°©ì‹
ë‘ ë²ˆì§¸ ì½”ë“œì˜ ì„±ê³µ ì›ë¦¬ë¥¼ ì ìš©í•œ ì‹¬í”Œ ë²„ì „
"""

import requests
import time
import random
import pandas as pd
from typing import List, Optional, Dict

# User-Agent ë¦¬ìŠ¤íŠ¸ (ëœë¤ ì„ íƒìš©)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0"
]


def fetch_reddit_simple(subreddit: str, limit: int = 25) -> Optional[Dict]:
    """
    ì‹¬í”Œí•˜ê²Œ Reddit JSON ê°€ì ¸ì˜¤ê¸°
    - User-Agentë§Œ ì‚¬ìš©
    - ë³µì¡í•œ í—¤ë” ì—†ìŒ
    - í•œ ë²ˆì— ìš”ì²­
    """
    url = f"https://www.reddit.com/r/{subreddit}/hot.json"
    
    # í—¤ë”ëŠ” User-Agent í•˜ë‚˜ë§Œ!
    headers = {
        "User-Agent": random.choice(USER_AGENTS)
    }
    
    params = {
        "limit": limit
    }
    
    try:
        # 429 ë°©ì§€ìš© ì§§ì€ ëŒ€ê¸°
        time.sleep(1)
        
        print(f"  ğŸ”— ìš”ì²­ URL: {url}")
        print(f"  ğŸ“‹ User-Agent: {headers['User-Agent'][:50]}...")
        
        response = requests.get(url, headers=headers, params=params, timeout=10)
        
        # ìƒì„¸ ë””ë²„ê¹… ì¶œë ¥
        print(f"  ğŸ“¡ ìƒíƒœì½”ë“œ: {response.status_code}")
        print(f"  ğŸ“¦ Content-Type: {response.headers.get('Content-Type', 'N/A')}")
        
        if response.status_code == 200:
            print(f"  âœ“ ì„±ê³µ!")
            return response.json()
        elif response.status_code == 403:
            print(f"  âœ— 403 Forbidden - ì ‘ê·¼ ê±°ë¶€ë¨")
            print(f"  ì‘ë‹µ: {response.text[:300]}")
        elif response.status_code == 429:
            print(f"  âœ— 429 Too Many Requests - ìš”ì²­ ì œí•œ ì´ˆê³¼")
            print(f"  ì‘ë‹µ: {response.text[:300]}")
        else:
            print(f"  âœ— ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœì½”ë“œ")
            print(f"  ì‘ë‹µ ë‚´ìš©: {response.text[:300]}")
        
        return None
            
    except requests.exceptions.Timeout:
        print(f"  âœ— Timeout ì—ëŸ¬: ì„œë²„ ì‘ë‹µ ì—†ìŒ (10ì´ˆ ì´ˆê³¼)")
        return None
    except requests.exceptions.ConnectionError:
        print(f"  âœ— ì—°ê²° ì—ëŸ¬: ì¸í„°ë„· ì—°ê²° í™•ì¸ í•„ìš”")
        return None
    except Exception as e:
        print(f"  âœ— ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
        return None


def crawl_reddit_natural(subreddits: List[str], max_posts_per_sub: int = 50) -> pd.DataFrame:
    """
    ìì—°ìŠ¤ëŸ¬ìš´ Reddit í¬ë¡¤ë§
    - ì¬ì‹œë„ ì—†ìŒ
    - ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°
    - ì‹¬í”Œí•œ ë¡œì§
    """
    all_posts = []
    
    for i, subreddit in enumerate(subreddits, 1):
        print(f"\n[{i}/{len(subreddits)}] r/{subreddit} ìˆ˜ì§‘ ì¤‘...")
        
        # í•œ ë²ˆë§Œ ì‹œë„
        data = fetch_reddit_simple(subreddit, limit=min(max_posts_per_sub, 100))
        
        if not data:
            print(f"  âœ— ê±´ë„ˆëœ€")
            continue
        
        # í¬ìŠ¤íŠ¸ ì¶”ì¶œ
        children = data.get("data", {}).get("children", [])
        
        if not children:
            print(f"  âœ— ë°ì´í„° ì—†ìŒ")
            continue
        
        collected = 0
        for child in children:
            post_data = child.get("data", {})
            
            # ê´‘ê³ ë‚˜ ê³ ì •ê¸€ ì œì™¸
            if post_data.get("stickied") or post_data.get("score", 0) < 10:
                continue
            
            all_posts.append({
                "subreddit": subreddit,
                "id": post_data.get("id"),
                "title": post_data.get("title", ""),
                "selftext": post_data.get("selftext", "")[:300],  # 300ìë§Œ
                "score": post_data.get("score", 0),
                "num_comments": post_data.get("num_comments", 0),
                "created_utc": post_data.get("created_utc", 0),
                "author": post_data.get("author", ""),
                "url": post_data.get("url", ""),
            })
            
            collected += 1
            if collected >= max_posts_per_sub:
                break
        
        print(f"  âœ“ {collected}ê°œ ìˆ˜ì§‘")
        
        # ì„œë¸Œë ˆë”§ ê°„ 1ì´ˆë§Œ ëŒ€ê¸° (ìì—°ìŠ¤ëŸ½ê²Œ)
        if i < len(subreddits):
            time.sleep(1)
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(all_posts)
    
    # Score ìˆœìœ¼ë¡œ ì •ë ¬
    if not df.empty:
        df = df.sort_values("score", ascending=False).reset_index(drop=True)
    
    return df


def main():
    """ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("Reddit ìì—°ìŠ¤ëŸ¬ìš´ í¬ë¡¤ëŸ¬")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸í•  ì„œë¸Œë ˆë”§
    test_subreddits = ["technology", "programming", "worldnews"]
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    df = crawl_reddit_natural(test_subreddits, max_posts_per_sub=30)
    
    if not df.empty:
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['id'], keep='first')
        
        # ì €ì¥
        output = "reddit_posts_natural.csv"
        df.to_csv(output, index=False, encoding="utf-8-sig")
        
        print(f"\n{'=' * 80}")
        print(f"âœ“ ì„±ê³µ! {len(df)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘")
        print(f"âœ“ íŒŒì¼: {output}")
        print(f"{'=' * 80}")
        
        # í†µê³„
        print("\nğŸ“Š ì„œë¸Œë ˆë”§ë³„ ìˆ˜ì§‘:")
        print(df['subreddit'].value_counts())
        
        print("\nğŸ”¥ ì¸ê¸° í¬ìŠ¤íŠ¸ TOP 5:")
        for idx, row in df.head(5).iterrows():
            print(f"  [{row['score']:>5} ì ] [{row['subreddit']:>15}] {row['title'][:50]}...")
        
    else:
        print(f"\n{'=' * 80}")
        print("âœ— ìˆ˜ì§‘ ì‹¤íŒ¨")
        print(f"{'=' * 80}")
        print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print("1. VPN ì¼œê³  ë‹¤ì‹œ ì‹œë„")
        print("2. ë‹¤ë¥¸ ì‹œê°„ëŒ€ì— ì‹œë„ (ë¯¸êµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ì „/ì˜¤í›„)")
        print("3. ì„œë¸Œë ˆë”§ ì´ë¦„ í™•ì¸ (ëŒ€ì†Œë¬¸ì, ì² ì)")
        print("4. IPê°€ ì¼ì‹œ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŒ â†’ 30ë¶„ í›„ ì¬ì‹œë„")


if __name__ == "__main__":
    main()