"""
Trend Scout Node - ë ˆë”§ íŠ¸ë Œë“œ ë°œêµ´ ì—ì´ì „íŠ¸ (JSON ë°©ì‹)
API í‚¤ ì—†ì´ ë ˆë”§ì˜ ê³µê°œ JSON URLì„ í†µí•´ ìµœì‹  íŠ¸ë Œë“œë¥¼ ìˆ˜ì§‘í•˜ê³ , 
ì±„ë„ í˜ë¥´ì†Œë‚˜ì— ë§ëŠ” ë‰´ìŠ¤ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""
from typing import Dict, Any, List, Optional
import logging
import requests
import random
import json
import time
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

# ë¸Œë¼ìš°ì € ìœ„ì¥ìš© User-Agent ë¦¬ìŠ¤íŠ¸ (429 ì°¨ë‹¨ ë°©ì§€)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0"
]

def trend_scout_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë ˆë”§ì—ì„œ íŠ¸ë Œë“œë¥¼ ìˆ˜ì§‘í•˜ê³  ë‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ë…¸ë“œ
    """
    logger.info("Trend Scout Node (JSON Mode) ì‹œì‘")

    # 1. ì…ë ¥ í™•ì¸ & íƒ€ê²ŸíŒ…
    channel_profile = state.get("channel_profile", {})
    interests = channel_profile.get("topics", [])
    
    target_subreddits = _determine_subreddits(interests)
    logger.info(f"íƒ€ê²Ÿ ì„œë¸Œë ˆë”§: {target_subreddits}")

    # 2. ë°ì´í„° ìˆ˜ì§‘ (HTTP Requests)
    raw_posts = _fetch_reddit_json(target_subreddits)
    logger.info(f"ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸: {len(raw_posts)}ê°œ")

    # ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì•ˆì „ì¥ì¹˜ (Fallback)
    if not raw_posts:
        logger.warning("ë ˆë”§ ìˆ˜ì§‘ ì‹¤íŒ¨ -> ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
        fallback_keywords = ["ìµœì‹  ë‰´ìŠ¤ íŠ¸ë Œë“œ", "ê¸€ë¡œë²Œ í•«ì´ìŠˆ", "IT ê¸°ìˆ  ë™í–¥"]
        if interests:
            fallback_keywords = [f"ìµœì‹  {i} ë‰´ìŠ¤" for i in interests]
        
        return {
            "researchPlan": {
                "newsQuery": fallback_keywords,
                "freshnessDays": 7
            }
        }

    # 3. LLM í•„í„°ë§ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
    final_keywords = _filter_and_extract_keywords(raw_posts, channel_profile)
    
    # [ê²°ê³¼ ë°˜í™˜]
    # í‚¤ì›Œë“œì™€ íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„°ë¥¼ ë°˜í™˜
    # topic: Plannerê°€ ì‚¬ìš©í•  ë©”ì¸ ì£¼ì œ (1ìˆœìœ„ í‚¤ì›Œë“œ ìë™ ì„ íƒ)
    selected_topic = final_keywords[0] if final_keywords else "Latest Tech Trends"
    if final_keywords:
        logger.info(f"ì„ ì •ëœ í‚¤ì›Œë“œ: {final_keywords}")
    
    return {
        "topic": selected_topic,
        "trend_analysis": {
            "keywords": final_keywords,
            "raw_posts": raw_posts, # ëŒ“ê¸€ ë²ˆì—­ë³¸ í¬í•¨ëœ í¬ìŠ¤íŠ¸ ë°ì´í„°
            "top_comments": [c for p in raw_posts for c in p.get("top_comments", [])] # ì „ì²´ ëŒ“ê¸€ í’€ ëª¨ìŒ (í•„ìš” ì‹œ)
        }
    }


def _determine_subreddits(interests: List[str]) -> List[str]:
    """ê´€ì‹¬ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ íƒìƒ‰í•  ì„œë¸Œë ˆë”§ ê²°ì •"""
    # ê¸°ë³¸ê°’ (í˜ë¥´ì†Œë‚˜ ì—†ì„ ë•Œ)
    if not interests:
        return ["popular", "worldnews", "todayilearned"]
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤í•‘ (í™•ì¥ ê°€ëŠ¥)
    mapping = {
        "AI": ["artificial", "technology", "singularity"],
        "Tech": ["technology", "gadgets", "hardware"],
        "Finance": ["investing", "stocks", "economics"],
        "Game": ["gaming", "Games", "pcgaming"],
        "Korea": ["korea", "Hangukin"],
        "General": ["popular", "worldnews"]
    }
    
    targets = set()
    for interest in interests:
        # ë§¤í•‘ëœ ê²Œ ìˆìœ¼ë©´ ì¶”ê°€, ì—†ìœ¼ë©´ ê´€ì‹¬ì‚¬ ìì²´ë¥¼ ì„œë¸Œë ˆë”§ìœ¼ë¡œ ì‹œë„
        found = False
        for key, subs in mapping.items():
            if key.lower() in interest.lower():
                targets.update(subs)
                found = True
        if not found:
            targets.add(interest.replace(" ", "")) # ê³µë°± ì œê±° í›„ ì‹œë„
            
    # ë„ˆë¬´ ë§ìœ¼ë©´ 3ê°œë§Œ, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    result = list(targets)[:3]
    if not result:
        return ["popular", "technology"]
    return result


def _fetch_reddit_json(subreddits: List[str], limit_per_sub: int = 25) -> List[Dict]:
    """JSON URLì„ í†µí•´ ê²Œì‹œê¸€ ìˆ˜ì§‘"""
    all_posts = []
    
    for sub in subreddits:
        url = f"https://www.reddit.com/r/{sub}/hot.json?limit={limit_per_sub}"
        headers = {"User-Agent": random.choice(USER_AGENTS)}
        
        try:
            # 429 ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(1) 
            
            res = requests.get(url, headers=headers, timeout=10)
            if res.status_code != 200:
                logger.warning(f"ìˆ˜ì§‘ ì‹¤íŒ¨ r/{sub}: Status {res.status_code}")
                continue
                
            data = res.json()
            children = data.get("data", {}).get("children", [])
            
            for child in children:
                post = child.get("data", {})
                
                # ê´‘ê³ (stickied)ë‚˜ ë„ˆë¬´ ì¸ê¸° ì—†ëŠ” ê¸€ ì œì™¸
                if post.get("stickied") or post.get("score", 0) < 10:
                    continue
                    
                all_posts.append({
                    "title": post.get("title"),
                    "score": post.get("score"),
                    "num_comments": post.get("num_comments"),
                    "url": post.get("url"),
                    "subreddit": sub,
                    "permalink": post.get("permalink"),
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¦„
                    "selftext": post.get("selftext", "")[:300]
                })
                
        except Exception as e:
            logger.warning(f"ì—ëŸ¬ ë°œìƒ r/{sub}: {e}")
            
    # ì „ì²´ì—ì„œ Score ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 5ê°œë§Œ ë‚¨ê¹€
    all_posts.sort(key=lambda x: x["score"], reverse=True)
    top_posts = all_posts[:5]  # ìƒìœ„ 5ê°œë§Œ ì§‘ì¤‘ ë¶„ì„

    # [ì œëª© ë²ˆì—­ ì¶”ê°€] ìƒìœ„ 5ê°œ í¬ìŠ¤íŠ¸ì˜ ì œëª©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
    try:
        import os
        if os.getenv("OPENAI_API_KEY"):
            llm_trans = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
            
            # ì œëª© ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            titles = [p["title"] for p in top_posts]
            titles_str = "\n".join([f"{i+1}. {t}" for i, t in enumerate(titles)])
            
            title_prompt = f"""
            Translate the following {len(titles)} Reddit titles into natural Korean.
            
            CRITICAL RULES:
            1. Keep technical terms, product names, and company names in English (e.g., "Sony", "DDR4", "Nvidia", "AI").
            2. Translate the rest into clear, natural Korean suitable for a news headline.
            3. Output must have exactly {len(titles)} lines. One input line = One output line.
            4. Return ONLY the translated lines.
            
            [Titles]
            {titles_str}
            """
            
            msg = HumanMessage(content=title_prompt)
            res = llm_trans.invoke([msg])
            translated_titles = [line for line in res.content.strip().split("\n") if line.strip()]
            
            # 1:1 ë§¤ì¹­í•˜ì—¬ ì œëª© êµì²´ (ê°œìˆ˜ ì•ˆ ë§ìœ¼ë©´ ê·¸ëƒ¥ ë‘ )
            if len(translated_titles) >= len(top_posts):
                for i, post in enumerate(top_posts):
                    # ë²ˆí˜¸(1. )ê°€ ë¶™ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì œê±° ì‹œë„
                    clean_title = translated_titles[i].split(". ", 1)[-1].strip()
                    # í˜¹ì‹œ ëª¨ë¥¼ 1. 2. ì œê±°ê°€ ì•ˆëœ ê²½ìš° ëŒ€ë¹„
                    if clean_title[0].isdigit() and clean_title[1] == '.':
                         clean_title = clean_title.split(". ", 1)[-1].strip()
                    
                    post["original_title"] = post["title"] # ì›ë¬¸ ë°±ì—…
                    post["title"] = clean_title
                    logger.debug(f"ì œëª© ë²ˆì—­: {post['original_title']} -> {post['title']}")

    except Exception as e:
        logger.warning(f"ì œëª© ë²ˆì—­ ì‹¤íŒ¨: {e}")

    # [ëŒ“ê¸€ ìˆ˜ì§‘ ì¶”ê°€] ìƒìœ„ 5ê°œ í¬ìŠ¤íŠ¸ì— ëŒ€í•´ì„œë§Œ ìƒì„¸ ëŒ“ê¸€ ìˆ˜ì§‘
    logger.info("ìƒìœ„ í¬ìŠ¤íŠ¸ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘...")
    for post in top_posts:
        try:
            # permalinkë¥¼ ì´ìš©í•´ ëŒ“ê¸€ JSON ìš”ì²­
            # ì˜ˆ: /r/technology/comments/1ab2c3/title.json
            permalink = post.get("permalink")
            if not permalink:
                continue
                
            comment_url = f"https://www.reddit.com{permalink}.json?sort=top&limit=5"
            headers = {"User-Agent": random.choice(USER_AGENTS)}
            
            time.sleep(1) # API ë§¤ë„ˆ ë”œë ˆì´
            
            c_res = requests.get(comment_url, headers=headers, timeout=10)
            if c_res.status_code == 200:
                c_data = c_res.json()
                # ëŒ“ê¸€ ë°ì´í„°ëŠ” ë°°ì—´ì˜ ë‘ ë²ˆì§¸ ìš”ì†Œì— ìˆìŒ
                if len(c_data) > 1:
                    comments_data = c_data[1].get("data", {}).get("children", [])
                    extracted_comments = []
                    extracted_comments_scores = []
                    
                    for c in comments_data:
                        c_body = c.get("data", {}).get("body")
                        c_score = c.get("data", {}).get("score", 0)
                        # ì‚­ì œëœ ëŒ“ê¸€ì´ë‚˜ ë‚´ìš© ì—†ëŠ” ê²ƒ ì œì™¸
                        if c_body and c_body != "[deleted]" and c_body != "[removed]":
                            extracted_comments.append(c_body) # í…ìŠ¤íŠ¸ë§Œ ì €ì¥
                            extracted_comments_scores.append(c_score) # ì ìˆ˜ ë”°ë¡œ ì €ì¥
                    
                    # [ë²ˆì—­] ì¶”ì¶œëœ ëŒ“ê¸€ì´ ìˆìœ¼ë©´ í•œêµ­ì–´ë¡œ ë²ˆì—­ (GPT-4o-mini)
                    if extracted_comments:
                        try:
                            # 5ê°œë§Œ ì¶”ë¦¼
                            target_comments = extracted_comments[:5]
                            target_scores = extracted_comments_scores[:5]
                            
                            import os
                            if os.getenv("OPENAI_API_KEY"):
                                llm_trans = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)
                                comments_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(target_comments)])
                                
                                trans_prompt = f"""
                                Translate the following {len(target_comments)} Reddit comments into natural Korean (informal community style).
                                Handle slang and idioms appropriately.
                                
                                IMPORTANT: 
                                - Output must have exactly {len(target_comments)} lines.
                                - Do NOT merge lines. One input line = One output line.
                                - Return ONLY the translated lines.
                                
                                [Comments]
                                {comments_str}
                                """
                                
                                msg = HumanMessage(content=trans_prompt)
                                trans_res = llm_trans.invoke([msg])
                                translated_list = [line for line in trans_res.content.strip().split("\n") if line.strip()]
                                
                                final_comments = []
                                # ê°œìˆ˜ê°€ ë‹¬ë¼ë„ ë²ˆì—­ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ìµœëŒ€í•œ ì‚¬ìš©
                                if translated_list:
                                    for i in range(len(target_comments)):
                                        score_str = f" (ğŸ‘{target_scores[i]})"
                                        if i < len(translated_list):
                                            # ë²ˆì—­ë¬¸ + ìŠ¤ì½”ì–´
                                            final_comments.append(translated_list[i].strip() + score_str)
                                        else:
                                            # ë²ˆì—­ ëª¨ìë¼ë©´ ì›ë¬¸ + ìŠ¤ì½”ì–´
                                            final_comments.append(target_comments[i] + score_str)
                                    
                                    post["top_comments"] = final_comments
                                else:
                                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ + ìŠ¤ì½”ì–´
                                    post["top_comments"] = [f"{c} (ğŸ‘{s})" for c, s in zip(target_comments, target_scores)]
                            else:
                                # API í‚¤ ì—†ì„ ë•Œ ì›ë¬¸ + ìŠ¤ì½”ì–´
                                post["top_comments"] = [f"{c} (ğŸ‘{s})" for c, s in zip(target_comments, target_scores)]
                                
                        except Exception as e:
                            logger.warning(f"ëŒ“ê¸€ ë²ˆì—­ ì‹¤íŒ¨: {e}")
                            # ì—ëŸ¬ ì‹œ fallback
                            post["top_comments"] = [f"{c} (ğŸ‘{s})" for c, s in zip(extracted_comments[:5], extracted_comments_scores[:5])]
                    else:
                        post["top_comments"] = []

                    logger.debug(f"ëŒ“ê¸€ ìˆ˜ì§‘ ë° ë²ˆì—­ ì™„ë£Œ: {post['title'][:20]}... ({len(post['top_comments'])}ê°œ)")
                    
        except Exception as e:
            logger.warning(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
            post["top_comments"] = []


    return top_posts


def _filter_and_extract_keywords(posts: List[Dict], persona: Dict) -> List[str]:
    """GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    # OpenAI ì„¤ì • í™•ì¸
    import os
    if not os.getenv("OPENAI_API_KEY"):
        logger.warning("OpenAI Key ì—†ìŒ -> ìƒìœ„ ì œëª© ë°˜í™˜")
        return [p["title"] for p in posts[:3]]

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    posts_text = ""
    for idx, p in enumerate(posts[:30]): # ìƒìœ„ 30ê°œë§Œ ë¶„ì„ ëŒ€ìƒ
        posts_text += f"{idx+1}. [{p['subreddit']}] {p['title']} (Score: {p['score']}, Comments: {p['num_comments']})\n"

    # í˜ë¥´ì†Œë‚˜ ì •ë³´ í¬ë§·íŒ…
    topics = persona.get("topics", ["General"])
    tone = persona.get("tone", "Informative")
    
    system_prompt = """
    You are a professional Content Researcher.
    Your goal is to select the BEST topics for a YouTube channel from the provided Reddit posts.
    
    CRITICAL INSTRUCTION:
    1. Select top 3-5 topics that match the Channel Persona.
    2. Convert them into **Korean Search Keywords** optimized for News Search (Naver/Google).
    3. Keywords must be **Noun-based** and **Factual**. (e.g., "Apple Vision Pro Release" -> "ì• í”Œ ë¹„ì „ í”„ë¡œ ì¶œì‹œ")
    4. Exclude memes, personal rants, or vague videos. Focus on specific events, products, or issues.
    
    Return ONLY a Python list of strings. Example: ["keyword1", "keyword2"]
    """
    
    user_prompt = f"""
    [Channel Persona]
    - Topics: {topics}
    - Tone: {tone}

    [Reddit Hot Posts]
    {posts_text}

    Extract 5 best news search keywords (Korean):
    """

    try:
        response = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ])
        
        content = response.content.strip()
        
        # íŒŒì‹± ì‹œë„ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
        import ast
        if content.startswith("[") and content.endswith("]"):
            return ast.literal_eval(content)
        
        # í¬ë§· ì•ˆ ë§ìœ¼ë©´ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì²˜ë¦¬
        return [line.strip("- *\"'") for line in content.split("\n") if line.strip()]

    except Exception as e:
        logger.error(f"LLM í•„í„°ë§ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ìƒìœ„ ê¸€ ì œëª© ê·¸ëƒ¥ ë°˜í™˜
        return [p["title"] for p in posts[:3]]
