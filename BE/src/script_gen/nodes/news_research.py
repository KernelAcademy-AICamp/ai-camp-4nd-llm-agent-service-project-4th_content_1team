"""
News Research Node (Advanced) - ê³ ì„±ëŠ¥ ë‰´ìŠ¤ ë° ì‹œê° ìë£Œ ìˆ˜ì§‘ê¸°

[í•µì‹¬ ë¡œì§]
1. Deep Fetch: ê²€ìƒ‰ì–´ë‹¹ 15ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í›„ë³´êµ°ì„ ë„“í˜
2. Smart Dedup: ìœ ì‚¬í•œ ì£¼ì œì˜ ê¸°ì‚¬ë¥¼ ê·¸ë£¹í•‘í•˜ê³ , ê° ê·¸ë£¹ì—ì„œ 'ì•Œì§œ ê¸°ì‚¬' 1ê°œì”©ë§Œ ì„ ë³„ (Top 3)
3. Aggressive Crawl: í˜ì´ì§€ ë‚´ 100px ì´ìƒ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘ (ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ ìµœì†Œí™”)
4. AI Context Check: GPT-4o-miniê°€ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ê³¼ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì§„ì§œ ì°¨íŠ¸/í‘œ ë°œêµ´
"""
from typing import Dict, Any, List, Optional
import requests
import json
import trafilatura
import logging
import concurrent.futures
import os
import hashlib
import re
import uuid
from datetime import datetime
from playwright.sync_api import sync_playwright
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

from app.core.config import settings

logger = logging.getLogger(__name__)

# ì„¤ì •
CRAWL_TIMEOUT = 40
MAX_WORKERS = 3 
SIMILARITY_THRESHOLD = 0.6  # ì œëª© ìœ ì‚¬ë„ ê¸°ì¤€ (0.6 ì´ìƒì´ë©´ ê°™ì€ ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼)


def news_research_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """ê° í‚¤ì›Œë“œë‹¹ ì œì¼ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ 1ê°œì”© ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    logger.info("News Research Node (Per-Keyword) ì‹œì‘")

    # 1. ì…ë ¥ ë°ì´í„° - Plannerì˜ research_plan í‚¤ì›Œë“œ ìš°ì„  ì‚¬ìš©
    content_brief = state.get("content_brief", {})
    research_plan = content_brief.get("researchPlan", {}) if content_brief else {}
    base_queries = research_plan.get("newsQuery", [])

    # fallback: Planner newsQueryê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ search_keywords ì‚¬ìš©
    if not base_queries:
        channel_profile = state.get("channel_profile", {})
        topic_context = channel_profile.get("topic_context", {})
        base_queries = topic_context.get("search_keywords", []) if topic_context else []
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ (Fallback): {base_queries}")
    else:
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ (Planner ì—­ì‚°): {base_queries}")

    if not base_queries:
        return {"news_data": {"articles": []}}

    topic = state.get("topic", "")
    logger.info(f"í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ ì‹œì‘: {len(base_queries)}ê°œ í‚¤ì›Œë“œ â†’ ê° 1ê°œ ê¸°ì‚¬")

    # 2. ê° í‚¤ì›Œë“œë‹¹ ì œì¼ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ 1ê°œì”© ì„ íƒ
    selected_articles = _fetch_one_per_keyword(base_queries, topic)
    logger.info(f"ì„ íƒëœ ê¸°ì‚¬: {len(selected_articles)}ê°œ")

    # 3. ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì •ë°€ í¬ë¡¤ë§ + AI ë¶„ì„
    full_articles = _crawl_and_analyze(selected_articles, topic=topic)
    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(full_articles)}ê°œ (ì„ íƒ {len(selected_articles)}ê°œ ì¤‘)")

    # í¬ë¡¤ë§ì— ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” Naver ê¸°ë³¸ ì •ë³´(ì œëª©/URL/ì„¤ëª…)ë¡œ í´ë°±
    crawled_urls = {art["url"] for art in full_articles}
    import hashlib
    for art in selected_articles:
        if art["url"] not in crawled_urls:
            logger.info(f"í¬ë¡¤ë§ ì‹¤íŒ¨ í´ë°±: {art['title'][:40]}")
            full_articles.append({
                **art,
                "id": hashlib.md5(art["url"].encode()).hexdigest(),
                "summary_short": art.get("desc", ""),
                "summary": art.get("desc", ""),
                "analysis": {"facts": [], "opinions": []},
                "images": [],
                "charts": [],
            })

    # 4. ê¸°ì‚¬ ì •ë ¬ (ì°¨íŠ¸ ìˆëŠ” ê¸°ì‚¬ ìš°ì„ )
    full_articles.sort(key=lambda x: (len(x.get("charts", [])), len(x.get("images", []))), reverse=True)

    # 5. ìˆ˜ì§‘ ê²°ê³¼ ë¡œê¹… (íŒ©íŠ¸/ì˜ê²¬ ì¶”ì¶œì€ article_analyzer_nodeì—ì„œ ìˆ˜í–‰)
    logger.info(f"[News Research] ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ: {len(full_articles)}ê°œ ê¸°ì‚¬")
    for idx, art in enumerate(full_articles, 1):
        title = (art.get("title") or "")[:60]
        source = art.get("source", "Unknown")
        content_len = len(art.get("content", ""))
        logger.info(f"  {idx}. [{source}] {title} (ë³¸ë¬¸ {content_len}ì)")

    return {
        "news_data": {
            "articles": full_articles,
            "structured_facts": [],    # article_analyzer_nodeì—ì„œ ì±„ì›€
            "structured_opinions": [], # article_analyzer_nodeì—ì„œ ì±„ì›€
            "queries_used": base_queries,
            "collected_at": datetime.now().isoformat()
        }
    }


def _log_research_result(
    articles: List[Dict],
    structured_facts: List[Dict],
    structured_opinions: List[str],
    queries_used: List[str],
) -> None:
    """News Research ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë¡œê¹…í•©ë‹ˆë‹¤."""
    lines = [
        "[News Research] ê²°ê³¼:",
        f"  ê²€ìƒ‰ ì¿¼ë¦¬: {queries_used}",
        f"  ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ",
        f"  íŒ©íŠ¸ ìˆ˜: {len(structured_facts)}ê°œ",
        f"  ì˜ê²¬ ìˆ˜: {len(structured_opinions)}ê°œ",
        "  ê¸°ì‚¬ ëª©ë¡:",
    ]
    for idx, art in enumerate(articles, 1):
        title = (art.get("title") or "")[:60]
        source = art.get("source", "Unknown")
        charts = art.get("charts", [])
        facts_cnt = len(art.get("analysis", {}).get("facts", []))
        lines.append(f"    {idx}. [{source}] {title}{'...' if len(art.get('title', '') or '') > 60 else ''}")
        url = art.get("url", "")
        url_display = (url[:80] + "...") if len(url) > 80 else url
        lines.append(f"       URL: {url_display} | íŒ©íŠ¸ {facts_cnt}ê°œ | ì°¨íŠ¸ {len(charts)}ê°œ")
    logger.info("\n".join(lines))


def _search_naver(endpoint: str, keyword: str, headers: dict, display: int = 10) -> List[Dict]:
    """
    Naver ê²€ìƒ‰ API í˜¸ì¶œ ê³µí†µ í•¨ìˆ˜.
    endpoint: "blog", "news", "cafearticle" ë“±
    """
    url = f"https://openapi.naver.com/v1/search/{endpoint}.json"
    try:
        params = {"query": keyword, "display": display, "sort": "sim"}
        res = requests.get(url, headers=headers, params=params, timeout=5)
        if res.status_code != 200:
            logger.warning(f"Naver {endpoint} API ì˜¤ë¥˜ ({keyword}): {res.status_code}")
            return []
        items = res.json().get("items", [])
        results = []
        for item in items:
            link = item.get("originallink") or item.get("link") or item.get("bloggerlink", "")
            if not link:
                continue
            clean_title = re.sub('<[^<]+?>', '', item.get("title", ""))
            clean_desc = re.sub('<[^<]+?>', '', item.get("description", ""))
            results.append({
                "title": clean_title,
                "url": link,
                "desc": clean_desc,
                "pub_date": item.get("pubDate") or item.get("postdate"),
                "query": keyword,
                "source": _extract_source_from_url(link),
                "_search_type": endpoint,
            })
        return results
    except Exception as e:
        logger.warning(f"Naver {endpoint} ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
        return []


def _fetch_one_per_keyword(keywords: List[str], topic: str) -> List[Dict]:
    """
    ê° í‚¤ì›Œë“œë‹¹ ì œì¼ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬/í¬ìŠ¤íŠ¸ 1ê°œì”© ì„ ë³„í•©ë‹ˆë‹¤.

    ê²€ìƒ‰ ìš°ì„ ìˆœìœ„:
      1ìˆœìœ„) Naver Blog  â€” ì‹¤ì‚¬ìš© ë¦¬ë·°, íŠœí† ë¦¬ì–¼, ë¹„êµ ê¸€ (how-to í‚¤ì›Œë“œì— ìµœì )
      2ìˆœìœ„) Naver News  â€” ì–¸ë¡ ì‚¬ ê¸°ì‚¬ (ì—…ê³„ ë™í–¥, í†µê³„, ì‚¬ê±´ ì¤‘ì‹¬)
    ê° ì†ŒìŠ¤ì—ì„œ GPTê°€ "ê´€ë ¨ì—†ìŒ" íŒë‹¨ ì‹œ ë‹¤ìŒ ì†ŒìŠ¤ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
    """
    results = []
    seen_urls: set = set()

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    if not client_id or not client_secret:
        logger.error("NAVER API Key Missing")
        return []

    api_key = os.getenv("OPENAI_API_KEY")
    llm = ChatOpenAI(model="gpt-4o", api_key=api_key, temperature=0) if api_key else None

    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}

    for raw_keyword in keywords:
        # ì‰¼í‘œë¡œ ì´ì–´ì§„ ë³µí•© í‚¤ì›Œë“œë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²« ë²ˆì§¸ ìœ íš¨í•œ ê²°ê³¼ ì‚¬ìš©
        sub_keywords = [k.strip() for k in raw_keyword.split(",") if k.strip()]
        if not sub_keywords:
            continue

        found = False
        for keyword in sub_keywords:
            if found:
                break

            # 1ìˆœìœ„: Naver Blog (ì‹¤ì‚¬ìš© ë¦¬ë·°/íŠœí† ë¦¬ì–¼ ì¤‘ì‹¬)
            blog_candidates = [
                c for c in _search_naver("blog", keyword, headers, display=10)
                if c["url"] not in seen_urls
            ]
            if blog_candidates:
                best = _pick_best_article(blog_candidates, keyword, topic, llm)
                if best:
                    seen_urls.add(best["url"])
                    results.append(best)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' [ë¸”ë¡œê·¸]: '{best['title'][:50]}' ì„ íƒ")
                    found = True
                    continue
                else:
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' [ë¸”ë¡œê·¸]: GPT ê´€ë ¨ì—†ìŒ íŒë‹¨ â†’ ë‰´ìŠ¤ë¡œ ì „í™˜")

            # 2ìˆœìœ„: Naver News (ì–¸ë¡ ì‚¬ ê¸°ì‚¬)
            news_candidates = [
                c for c in _search_naver("news", keyword, headers, display=10)
                if c["url"] not in seen_urls
            ]
            if news_candidates:
                best = _pick_best_article(news_candidates, keyword, topic, llm)
                if best:
                    seen_urls.add(best["url"])
                    results.append(best)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' [ë‰´ìŠ¤]: '{best['title'][:50]}' ì„ íƒ")
                    found = True
                    continue
                else:
                    logger.warning(f"í‚¤ì›Œë“œ '{keyword}' [ë‰´ìŠ¤]: GPT ê´€ë ¨ì—†ìŒ íŒë‹¨ â†’ ê¸°ì‚¬ ì—†ìŒ ì²˜ë¦¬")

    return results


def _pick_best_article(candidates: List[Dict], keyword: str, topic: str, llm) -> Optional[Dict]:
    """
    ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ì˜ìƒ ì£¼ì œì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê¸€ 1ê°œë¥¼ GPTë¡œ ì„ íƒí•©ë‹ˆë‹¤.
    ëª¨ë“  í›„ë³´ê°€ ê´€ë ¨ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤ (ê±°ë¶€ ê°€ëŠ¥).
    """
    if not candidates:
        return None
    if not llm:
        return candidates[0]

    try:
        article_list = "\n".join(
            f"{i+1}. {art['title']} â€” {art['desc'][:150]}"
            for i, art in enumerate(candidates)
        )

        prompt = f"""ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ ë¦¬ì„œì¹˜ë¥¼ ìœ„í•´ ê°€ì¥ ì í•©í•œ ê¸€ 1ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”.

[ì˜ìƒ ì£¼ì œ]
"{topic}"

[ê²€ìƒ‰ í‚¤ì›Œë“œ]
"{keyword}"

[í›„ë³´ ê¸€]
{article_list}

ì„ íƒ ê¸°ì¤€:
- ê²€ìƒ‰ í‚¤ì›Œë“œ "{keyword}"ì˜ í•µì‹¬ ë‚´ìš©ì„ ì§ì ‘ ë‹¤ë£¨ëŠ” ê¸€
- ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ì— ì¸ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì‚¬ì‹¤, ì‚¬ë¡€, ì‚¬ìš© ê²½í—˜ì´ ìˆëŠ” ê¸€
- ì‹¤ì‚¬ìš© ë¦¬ë·°, íŠœí† ë¦¬ì–¼, ë¹„êµ ê¸€ ìš°ì„ 
- ê´‘ê³ ì„±Â·í™ë³´ì„± ê¸€ ì œì™¸

âš ï¸ ì¤‘ìš”: í›„ë³´ ê¸€ ì¤‘ í‚¤ì›Œë“œ "{keyword}"ì™€ ì§ì ‘ ê´€ë ¨ëœ ê¸€ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´ ë°˜ë“œì‹œ "0"ì„ ë‹µí•˜ì„¸ìš”.
ì˜ˆë¥¼ ë“¤ì–´ "Copilot ì‚¬ìš©ë²•" í‚¤ì›Œë“œì¸ë° ë„ë¡œê³µì‚¬ë‚˜ ì£¼ì‹ ê¸°ì‚¬ë§Œ ìˆë‹¤ë©´ â†’ 0

ìˆ«ìë§Œ ì‘ë‹µí•˜ì„¸ìš”. ê´€ë ¨ ì—†ìœ¼ë©´ 0, ê´€ë ¨ ìˆìœ¼ë©´ í•´ë‹¹ ë²ˆí˜¸ (ì˜ˆ: 3)"""

        response = llm.invoke(prompt)
        idx_str = response.content.strip()

        # ìˆ«ìë§Œ ì¶”ì¶œ
        digits = re.sub(r'[^\d]', '', idx_str)
        if digits:
            idx = int(digits) - 1
            if idx == -1:
                # GPTê°€ "0" ë°˜í™˜ = ê´€ë ¨ì—†ìŒ ê±°ë¶€
                logger.info(f"ê¸°ì‚¬ ì„ íƒ: GPTê°€ '{keyword}' ê´€ë ¨ ê¸€ ì—†ìŒ íŒë‹¨ â†’ None ë°˜í™˜")
                return None
            if 0 <= idx < len(candidates):
                return candidates[idx]

        return None  # íŒŒì‹± ì‹¤íŒ¨ ì‹œë„ None (ë¶ˆí™•ì‹¤í•œ ê²°ê³¼ í¬í•¨ ì•ˆ í•¨)

    except Exception as e:
        logger.warning(f"ê¸°ì‚¬ ì„ íƒ ì‹¤íŒ¨ ({keyword}): {e} â†’ None ë°˜í™˜")
        return None



# ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ëª… ë§¤í•‘
SOURCE_DOMAIN_MAP = {
    "chosun.com": "ì¡°ì„ ì¼ë³´", "donga.com": "ë™ì•„ì¼ë³´", "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
    "hani.co.kr": "í•œê²¨ë ˆ", "khan.co.kr": "ê²½í–¥ì‹ ë¬¸", "kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
    "seoul.co.kr": "ì„œìš¸ì‹ ë¬¸", "munhwa.com": "ë¬¸í™”ì¼ë³´", "segye.com": "ì„¸ê³„ì¼ë³´",
    "mk.co.kr": "ë§¤ì¼ê²½ì œ", "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´", "hankyung.com": "í•œêµ­ê²½ì œ",
    "sedaily.com": "ì„œìš¸ê²½ì œ", "edaily.co.kr": "ì´ë°ì¼ë¦¬", "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "etnews.com": "ì „ìì‹ ë¬¸", "zdnet.co.kr": "ZDNet Korea",
    "bloter.net": "ë¸”ë¡œí„°", "ddaily.co.kr": "ë””ì§€í„¸ë°ì¼ë¦¬",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "yonhapnews.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "newsis.com": "ë‰´ì‹œìŠ¤", "news1.kr": "ë‰´ìŠ¤1",
    "bbc.com": "BBC", "bbc.co.uk": "BBC",
    "reuters.com": "Reuters", "bloomberg.com": "Bloomberg",
    "nytimes.com": "NYT", "wsj.com": "WSJ",
    "techcrunch.com": "TechCrunch", "theverge.com": "The Verge",
    "cnbc.com": "CNBC", "ft.com": "FT",
    "fortunekorea.co.kr": "í¬ì¶˜ì½”ë¦¬ì•„", "venturesquare.net": "ë²¤ì²˜ìŠ¤í€˜ì–´",
    "newspim.com": "ë‰´ìŠ¤í•Œ", "theinformation.com": "The Information",
    "inews24.com": "ì•„ì´ë‰´ìŠ¤24", "zdnet.com": "ZDNet",
}

def _extract_source_from_url(url: str) -> str:
    """URL ë„ë©”ì¸ì—ì„œ ì¶œì²˜ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        # ê³µí†µ ì„œë¸Œë„ë©”ì¸ ì œê±°
        for prefix in ("www.", "view.", "news.", "m.", "mobile."):
            if domain.startswith(prefix):
                domain = domain[len(prefix):]

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸/ì¹´í˜ íŠ¹ë³„ ì²˜ë¦¬
        if "blog.naver.com" in domain or "blog.me" in domain:
            return "ë„¤ì´ë²„ ë¸”ë¡œê·¸"
        if "cafe.naver.com" in domain:
            return "ë„¤ì´ë²„ ì¹´í˜"
        if "tistory.com" in domain:
            return "í‹°ìŠ¤í† ë¦¬"
        if "velog.io" in domain:
            return "Velog"
        if "brunch.co.kr" in domain:
            return "ë¸ŒëŸ°ì¹˜"

        # ì •í™•í•œ ë§¤ì¹­
        if domain in SOURCE_DOMAIN_MAP:
            return SOURCE_DOMAIN_MAP[domain]
        # ë¶€ë¶„ ë§¤ì¹­ (ì„œë¸Œë„ë©”ì¸ ëŒ€ì‘)
        for key, name in SOURCE_DOMAIN_MAP.items():
            if key in domain:
                return name
        return ""
    except Exception:
        return ""


import base64
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager

class LegacySSLAdapter(HTTPAdapter):
    """ì˜¤ë˜ëœ ì„œë²„(SSL Legacy Renegotiation) ì ‘ì†ì„ ìœ„í•œ ì–´ëŒ‘í„°"""
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context()
        ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

import uuid

def download_image_to_local(image_url: str, referrer_url: str = None) -> Optional[str]:
    """
    ì´ë¯¸ì§€ë¥¼ public/images/news ê²½ë¡œì— ì €ì¥í•˜ê³  ìƒëŒ€ ê²½ë¡œë¥¼ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    try:
        # BE í´ë” ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ìƒì„± (í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€)
        be_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        save_dir = os.path.join(be_root, "public", "images", "news")
        os.makedirs(save_dir, exist_ok=True)

        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": referrer_url if referrer_url else ""
        }
        
        res = session.get(image_url, headers=headers, timeout=10)
        if res.status_code != 200:
            return None
            
        ext = "jpg"
        if "png" in image_url.lower(): ext = "png"
        if "gif" in image_url.lower(): ext = "gif"
        
        filename = f"{uuid.uuid4()}.{ext}"
        filepath = os.path.join(save_dir, filename)
        
        with open(filepath, "wb") as f:
            f.write(res.content)
            
        return f"/images/news/{filename}"
    except Exception as e:
        logger.error(f"Image Download Failed: {e}")
        return None

import time

def _check_image_context(image_url: str, article_title: str, article_summary: str, referrer_url: str = None) -> Dict:
    """
    GPT-4o-miniì—ê²Œ [ê¸°ì‚¬ ìš”ì•½ + ì´ë¯¸ì§€(Base64)]ë¥¼ ë³´ì—¬ì£¼ê³  íŒë‹¨í•˜ê²Œ í•¨
    (Legacy SSL ì„œë²„ ì§€ì› + Rate Limit ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
    """
    max_retries = 3
    retry_delay = 2  # ì´ˆ
    
    for attempt in range(max_retries):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key: return {"relevant": False}
            
            # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (Legacy SSL ì§€ì› Session ì‚¬ìš©)
            session = requests.Session()
            session.mount('https://', LegacySSLAdapter())
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": referrer_url if referrer_url else ""
            }
            
            img_res = session.get(image_url, headers=headers, timeout=10)
            if img_res.status_code != 200:
                return {"relevant": False}
                
            # 2. Base64 ì¸ì½”ë”©
            encoded_string = base64.b64encode(img_res.content).decode("utf-8")
            data_url = f"data:image/jpeg;base64,{encoded_string}"
            
            llm = ChatOpenAI(model="gpt-4o", api_key=api_key)
            
            prompt = f"""
            [ë¶„ì„ ìš”ì²­]
            ê¸°ì‚¬ ì œëª©: {article_title}
            ê¸°ì‚¬ ìš”ì•½: {article_summary}
            
            ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì„œ JSONìœ¼ë¡œ ë‹µí•´ì¤˜:
            1. relevant: ì´ ì´ë¯¸ì§€ê°€ ê¸°ì‚¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. type: "chart", "table", "photo", "other"
            3. description: ì´ë¯¸ì§€ ì„¤ëª… (í•œê¸€)
            
            íŒë‹¨ ê¸°ì¤€:
            - chart/table: ê¸°ì‚¬ì˜ ë°ì´í„°/í†µê³„ë¥¼ ì‹œê°í™”í•œ ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ
            - photo: ê¸°ì‚¬ ì£¼ì œì™€ ì§ì ‘ ì—°ê´€ëœ ì‚¬ì§„
              ì˜ˆ) ë¶€ë™ì‚° ê¸°ì‚¬ â†’ ì•„íŒŒíŠ¸/ê±´ë¬¼ ì‚¬ì§„ OK
              ì˜ˆ) ìŠ¤í¬ì¸  ê¸°ì‚¬ â†’ ì„ ìˆ˜/ê²½ê¸° ì‚¬ì§„ OK
              ì˜ˆ) ê²½ì œ ê¸°ì‚¬ â†’ ê´€ë ¨ í˜„ì¥/ì¸ë¬¼ ì‚¬ì§„ OK
            - other: ê´‘ê³ , ë¡œê³ , ë°°ë„ˆ, ì•„ì´ì½˜ â†’ relevant=false
            
            relevant=true ì¡°ê±´:
            - ì°¨íŠ¸/í‘œëŠ” ë¬´ì¡°ê±´ í¬í•¨
            - ì‚¬ì§„ì€ ê¸°ì‚¬ ì£¼ì œì™€ ëª…í™•íˆ ì—°ê´€ëœ ê²½ìš°ë§Œ í¬í•¨
            - ê´‘ê³ /ë¡œê³ /ë°°ë„ˆ/ê¸°ì í”„ë¡œí•„ ì‚¬ì§„ì€ ì œì™¸
            """
            
            msg = HumanMessage(content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_url}}
            ])
            
            res = llm.invoke([msg])
            content = res.content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
            
        except Exception as e:
            if "rate_limit" in str(e).lower() or "429" in str(e):
                if attempt < max_retries - 1:
                    logger.warning(f"Rate limit hit, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    continue
            logger.warning(f"AI Check Error: {e}")
            return {"relevant": False}
    return {"relevant": False}


def _optimize_crawl_url(url: str) -> str:
    """
    í¬ë¡¤ë§í•˜ê¸° ì–´ë ¤ìš´ URLì„ ì ‘ê·¼ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ë„¤ì´ë²„ ë¸”ë¡œê·¸: ë°ìŠ¤í¬íƒ‘(iframe êµ¬ì¡°) â†’ ëª¨ë°”ì¼(ë³¸ë¬¸ ì§ì ‘ ë…¸ì¶œ)
    - ë„¤ì´ë²„ ì¹´í˜: ë°ìŠ¤í¬íƒ‘ â†’ ëª¨ë°”ì¼
    """
    if not url:
        return url
    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ desktop â†’ mobile (iframe ë¬¸ì œ í•´ê²°)
    if "blog.naver.com" in url and "m.blog.naver.com" not in url:
        return url.replace("blog.naver.com", "m.blog.naver.com")
    # ë„¤ì´ë²„ ì¹´í˜ desktop â†’ mobile
    if "cafe.naver.com" in url and "m.cafe.naver.com" not in url:
        return url.replace("cafe.naver.com", "m.cafe.naver.com")
    return url


def _extract_text_fallback(html: str) -> str:
    """
    trafilatura ì‹¤íŒ¨ ì‹œ BeautifulSoupìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° í›„ ì˜ë¯¸ ìˆëŠ” ì¤„ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")
        for tag in soup(["script", "style", "nav", "header", "footer",
                         "aside", "noscript", "iframe", "form"]):
            tag.decompose()
        raw = soup.get_text(separator="\n", strip=True)
        lines = [l.strip() for l in raw.split("\n") if len(l.strip()) > 20]
        return "\n".join(lines)
    except Exception:
        return ""


def _crawl_and_analyze(articles: List[Dict], topic: str = "") -> List[Dict]:
    """Playwrightë¡œ ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ë¥¼ ì‹¹ ê¸ì–´ì˜¤ê³  AIë¡œ ë¶„ì„"""
    results = []
    
    if not articles:
        return []

    def process_one(item):
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # ğŸ­ ë´‡ íƒì§€ ìš°íšŒ: User-Agent ë³€ê²½
                page = browser.new_page(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                
                # ë¡œë”© ëŒ€ê¸° (í¬ë¡¤ë§ ìµœì í™” URL ì‚¬ìš©)
                crawl_url = _optimize_crawl_url(item["url"])
                try:
                    page.goto(crawl_url, timeout=CRAWL_TIMEOUT*1000, wait_until="domcontentloaded")

                    # [Scroll Logic] Lazy Loading ì´ë¯¸ì§€ ë¡œë”©ì„ ìœ„í•´ ìŠ¤í¬ë¡¤ ë‹¤ìš´
                    for _ in range(5):
                        page.evaluate("window.scrollBy(0, document.body.scrollHeight / 5)")
                        page.wait_for_timeout(2000)  # 2.0ì´ˆ ëŒ€ê¸° (Wait longer for lazy loading)

                except:
                    browser.close()
                    return None

                # â”€â”€ ë³¸ë¬¸ ì¶”ì¶œ (3ë‹¨ê³„ í´ë°±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                content_html = page.content()

                # 1ë‹¨ê³„: trafilatura (favor_recall=True â†’ ë” ë§ì€ í…ìŠ¤íŠ¸ íšŒìˆ˜)
                text = trafilatura.extract(
                    content_html,
                    include_links=False,
                    no_fallback=False,
                    favor_recall=True,
                )

                # 2ë‹¨ê³„: ë„¤ì´ë²„ ë¸”ë¡œê·¸ iframe ë‚´ë¶€ ì§ì ‘ ì¶”ì¶œ
                if (not text or len(text) < 50) and "naver.com" in item["url"]:
                    try:
                        iframe = page.query_selector("iframe#mainFrame, iframe.se-main-section, #mainFrame")
                        if iframe:
                            frame = iframe.content_frame()
                            if frame:
                                frame.wait_for_load_state("domcontentloaded", timeout=10000)
                                iframe_html = frame.content()
                                text = trafilatura.extract(
                                    iframe_html,
                                    include_links=False,
                                    no_fallback=False,
                                    favor_recall=True,
                                )
                                if text and len(text) >= 50:
                                    content_html = iframe_html  # ì´ë¯¸ì§€ ì¶”ì¶œë„ iframe ê¸°ì¤€ìœ¼ë¡œ
                                    logger.info(f"[Crawl] ë„¤ì´ë²„ iframe ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {item['url'][:60]}")
                    except Exception as iframe_err:
                        logger.debug(f"[Crawl] iframe ì¶”ì¶œ ì‹¤íŒ¨: {iframe_err}")

                # 3ë‹¨ê³„: BeautifulSoup í´ë°±
                if not text or len(text) < 50:
                    text = _extract_text_fallback(content_html)
                    if text and len(text) >= 50:
                        logger.info(f"[Crawl] BeautifulSoup í´ë°± ì„±ê³µ: {item['url'][:60]}")

                if not text or len(text) < 50:
                    logger.warning(f"[Crawl] ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (3ë‹¨ê³„ ëª¨ë‘ ì‹¤íŒ¨): {item['url'][:60]}")
                    browser.close()
                    return None
                
                # [ID ìƒì„±] URL í•´ì‹œ ê¸°ë°˜ ê³ ìœ  ID ë¶€ì—¬ (Verifier ì—°ê²°ìš©)
                item["id"] = hashlib.md5(item["url"].encode()).hexdigest()

                # [ì¶œì²˜ëª… ìë™ ì¶”ì¶œ] og:site_name ë©”íƒ€íƒœê·¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ê°€ì ¸ì˜¤ê¸°
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(content_html, "html.parser")
                    og_tag = soup.find("meta", property="og:site_name")
                    if og_tag and og_tag.get("content", "").strip():
                        item["og_source"] = og_tag["content"].strip()
                except Exception:
                    pass

                # ì´ë¯¸ì§€ ì¶”ì¶œ (Lazy Loading ì§€ì› + Aggressive Mode)
                # data-src, data-original, data-url ìš°ì„  í™•ì¸
                images_found = []
                
                # [ê°œì„ ] ìŠ¤ë§ˆíŠ¸ ë³¸ë¬¸ ì˜ì—­ ê°ì§€ ì•Œê³ ë¦¬ì¦˜
                def find_article_body_smart(page):
                    """
                    íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ì„ ìë™ ê°ì§€
                    - í…ìŠ¤íŠ¸ ë°€ë„, ë¬¸ë‹¨ ê°œìˆ˜, ì´ë¯¸ì§€ ë“±ì„ ì¢…í•© í‰ê°€
                    """
                    # 1ë‹¨ê³„: ê¸°ì¡´ ì„ íƒì ì‹œë„ (ë¹ ë¥¸ ê²½ë¡œ)
                    common_selectors = [
                        "article", 
                        ".article-body", ".article_body", 
                        "#articleBody", "#newsBody",
                        "div[itemprop='articleBody']",
                    ]
                    
                    for selector in common_selectors:
                        try:
                            elem = page.query_selector(selector)
                            if elem and len(elem.inner_text()) > 200:
                                logger.debug(f"[ARTICLE] ê¸°ì¡´ ì„ íƒìë¡œ ë³¸ë¬¸ ë°œê²¬: {selector}")
                                return elem
                        except:
                            continue
                    
                    # 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ - í›„ë³´ ìš”ì†Œ ìˆ˜ì§‘
                    logger.debug("[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ íƒìƒ‰ ì‹œì‘")
                    candidates = page.query_selector_all('div, section, article')
                    
                    best_score = -999999
                    best_elem = None
                    
                    for elem in candidates:
                        try:
                            text = elem.inner_text()
                            text_len = len(text)
                            
                            # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                            if text_len < 200:
                                continue
                            
                            # ì ìˆ˜ ê³„ì‚° ìš”ì†Œ
                            p_count = len(elem.query_selector_all('p'))
                            li_count = len(elem.query_selector_all('li'))
                            img_count = len(elem.query_selector_all('img'))
                            link_count = len(elem.query_selector_all('a'))
                            
                            # ì ìˆ˜ = í…ìŠ¤íŠ¸ ê¸¸ì´ + ë¬¸ë‹¨ + ì´ë¯¸ì§€ - ë§í¬
                            score = (text_len * 0.3) + ((p_count + li_count) * 100) + (img_count * 50) - (link_count * 10)
                            
                            logger.debug(f"[ARTICLE] í›„ë³´: text={text_len}, p={p_count}, img={img_count}, link={link_count}, score={score:.0f}")
                            
                            if score > best_score:
                                best_score = score
                                best_elem = elem
                        except:
                            continue
                    
                    if best_elem:
                        logger.info(f"[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ë°œê²¬ (ì ìˆ˜: {best_score:.0f})")
                        return best_elem
                    
                    # 3ë‹¨ê³„: í´ë°± - body ì „ì²´
                    logger.warning("[ARTICLE] ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨. body ì „ì²´ ì‚¬ìš©")
                    return page.query_selector('body')
                
                # ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                article_area = find_article_body_smart(page)
                
                # ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì•˜ìœ¼ë©´ ê·¸ ì•ˆì—ì„œë§Œ, ëª» ì°¾ì•˜ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰
                search_area = article_area if article_area else page
                
                # 1. img íƒœê·¸
                imgs = search_area.query_selector_all("img")
                logger.debug(f"[ARTICLE] ê²€ìƒ‰ ì˜ì—­ì—ì„œ ë°œê²¬í•œ img íƒœê·¸: {len(imgs)}ê°œ")
                
                for img in imgs:
                    # Lazy Loading ì†ì„± í™•ì¸
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                        # [Filter] ì“°ë ˆê¸° ì´ë¯¸ì§€ ì œê±° (ê¸°ì, ì•„ì´ì½˜, ë°°ë„ˆ ë“±)
                        src_lower = src.lower()
                        trash_keywords = [
                            '.svg', '.gif', 'logo', 'icon', 'banner', 'ad', 'button', 'btn',
                            'reporter', 'profile', 'journalist', 'avatar'  # ê¸°ì/í”„ë¡œí•„ ì‚¬ì§„ í•„í„° ì¶”ê°€
                        ]
                        if any(x in src_lower for x in trash_keywords):
                            logger.debug(f"[FILTER] í‚¤ì›Œë“œ ì°¨ë‹¨: {src[:80]}")
                            continue
                            
                        # í¬ê¸° ì²´í¬ (JS)
                        try:
                            w = img.evaluate("el => el.naturalWidth")
                            h = img.evaluate("el => el.naturalHeight")
                            
                            # [Filter] í¬ê¸° ê¸°ì¤€ ìƒí–¥ (50px -> 150px)
                            # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì •ë³´ê°€ì¹˜ê°€ ì—†ìŒ
                            if w > 0 and w < 150 and h > 0 and h < 150:
                                logger.debug(f"[FILTER] í¬ê¸° ì°¨ë‹¨: {src[:80]} ({w}x{h})")
                                continue
                                
                            # Lazy Loading ì´ˆê¸°í™” ì „ì´ë¼ 0ì¼ ìˆ˜ë„ ìˆìŒ -> ì¼ë‹¨ URL ë¯¿ê³  ìˆ˜ì§‘ (AIê°€ ìµœì¢… íŒë³„)
                            images_found.append({"url": src, "width": w, "height": h})
                        except:
                            # í¬ê¸° í™•ì¸ ì‹¤íŒ¨í•´ë„ URLì´ ì •ìƒì´ë©´ ì¼ë‹¨ ì¶”ê°€
                            images_found.append({"url": src, "width": 0, "height": 0})
                            
                # 2. figure ì•ˆì˜ ì´ë¯¸ì§€ (ë³´í†µ ì¤‘ìš”í•œ ê¸°ì‚¬ ì´ë¯¸ì§€)
                figures = search_area.query_selector_all("figure img")
                for img in figures:
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                         images_found.append({"url": src, "width": 0, "height": 0})

                # [DEBUG] ì´ë¯¸ì§€ ë°œê²¬ ì§í›„ ë¡œê¹…
                logger.info(f"[DEBUG] {item['url']} - ì›ë³¸ ì´ë¯¸ì§€ ë°œê²¬: {len(images_found)}ê°œ")
                for idx, img in enumerate(images_found[:10]):
                    logger.info(f"  [{idx+1}] {img['url'][:100]}... (w:{img['width']}, h:{img['height']})")
                
                # ì¤‘ë³µ URL ì œê±°
                seen_urls = set()
                candidates = []
                for img in images_found:
                    if img["url"] not in seen_urls:
                        candidates.append(img)
                        seen_urls.add(img["url"])
                
                # [DEBUG] ì¤‘ë³µ ì œê±° í›„ ë¡œê¹…
                logger.info(f"[DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(candidates)}ê°œ")
                
                # --- AI ë¶„ì„ ë‹¨ê³„ (Context check) - ë¹„í™œì„±í™” ì‹œ ìŠ¤í‚µ â”€â”€
                final_images = []
                charts = []
                
                if settings.news_image_analysis_enabled:
                    # ê¸°ì‚¬ ìš”ì•½ (ì•ë¶€ë¶„ 500ì) - AIì—ê²Œ ë¬¸ë§¥ ì œê³µìš©
                    summary = text[:500]
                    target_images = candidates[:5]
                    logger.info(f"[DEBUG] AI ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {len(target_images)}ê°œ")

                    def analyze_single_image(img):
                        analysis = _check_image_context(img["url"], item["title"], summary, referrer_url=item["url"])
                        if analysis.get("relevant"):
                            img_data = {
                                "url": img["url"],
                                "width": img.get("width", 0),
                                "height": img.get("height", 0),
                                "type": analysis.get("type", "other"),
                                "desc": analysis.get("description", "")
                            }
                            local_path = download_image_to_local(img["url"], item["url"])
                            if local_path:
                                img_data["url"] = local_path
                            return (analysis.get("type"), img_data)
                        return None

                    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as img_executor:
                        results = list(img_executor.map(analyze_single_image, target_images))
                    for result in results:
                        if result:
                            img_type, img_data = result
                            if img_type in ["chart", "table"]:
                                charts.append(img_data)
                            else:
                                final_images.append(img_data)
                else:
                    logger.info("[News Research] ì´ë¯¸ì§€ AI ë¶„ì„ ë¹„í™œì„±í™” â†’ ìŠ¤í‚µ")
                
                browser.close()
                
                # ì¶œì²˜ëª… ì¶”ì¶œ (URL ë§µ â†’ og:site_name ìˆœì„œ, GPT ì—†ì´)
                url_source = _extract_source_from_url(item.get("url", ""))
                og_source = item.get("og_source", "")
                item["source"] = url_source or og_source or "Unknown"

                # íŒ©íŠ¸Â·ì˜ê²¬ ì¶”ì¶œì€ article_analyzer_nodeì—ì„œ ìˆ˜í–‰
                item["analysis"] = {"facts": [], "opinions": []}
                item["summary_short"] = item.get("desc", "") or text[:200]
                item["summary"] = text[:3000]  # Writerê°€ ì°¸ê³ í•  ì›ë¬¸ ìœ ì§€

                item["content"] = text
                item["images"] = final_images
                item["charts"] = charts
                return item
                
        except Exception as e:
            logger.error(f"Crawl failed {item['url']}: {e}")
            return None

    # ë³‘ë ¬ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_one, item): item for item in articles}
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: results.append(res)
            
    return results
