"""
News Research Node (Advanced) - ê³ ì„±ëŠ¥ ë‰´ìŠ¤ ë° ì‹œê° ìë£Œ ìˆ˜ì§‘ê¸°

[í•µì‹¬ ë¡œì§]
1. Deep Fetch: ê²€ìƒ‰ì–´ë‹¹ 15ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í›„ë³´êµ°ì„ ë„“í˜
2. Smart Dedup: ìœ ì‚¬í•œ ì£¼ì œì˜ ê¸°ì‚¬ë¥¼ ê·¸ë£¹í•‘í•˜ê³ , ê° ê·¸ë£¹ì—ì„œ 'ì•Œì§œ ê¸°ì‚¬' 1ê°œì”©ë§Œ ì„ ë³„ (Top 3)
3. Aggressive Crawl: í˜ì´ì§€ ë‚´ 100px ì´ìƒ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘ (ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ ìµœì†Œí™”)
4. AI Context Check: GPT-4o-miniê°€ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ê³¼ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì§„ì§œ ì°¨íŠ¸/í‘œ ë°œêµ´
"""
from typing import Dict, Any, List, Optional
import requests
import json
import trafilatura
import logging
import concurrent.futures
import os
import hashlib
import re
import uuid
from datetime import datetime
from playwright.sync_api import sync_playwright
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

# ì„¤ì •
CRAWL_TIMEOUT = 40
MAX_WORKERS = 3 
SIMILARITY_THRESHOLD = 0.6  # ì œëª© ìœ ì‚¬ë„ ê¸°ì¤€ (0.6 ì´ìƒì´ë©´ ê°™ì€ ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼)


def news_research_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """ê° í‚¤ì›Œë“œë‹¹ ì œì¼ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ 1ê°œì”© ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    logger.info("News Research Node (Per-Keyword) ì‹œì‘")

    # 1. ì…ë ¥ ë°ì´í„° - Plannerì˜ research_plan í‚¤ì›Œë“œ ìš°ì„  ì‚¬ìš©
    content_brief = state.get("content_brief", {})
    research_plan = content_brief.get("researchPlan", {}) if content_brief else {}
    base_queries = research_plan.get("newsQuery", [])

    # fallback: Planner newsQueryê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ search_keywords ì‚¬ìš©
    if not base_queries:
        channel_profile = state.get("channel_profile", {})
        topic_context = channel_profile.get("topic_context", {})
        base_queries = topic_context.get("search_keywords", []) if topic_context else []
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ (Fallback): {base_queries}")
    else:
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ (Planner ì—­ì‚°): {base_queries}")

    if not base_queries:
        return {"news_data": {"articles": []}}

    topic = state.get("topic", "")
    logger.info(f"í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ê²€ìƒ‰ ì‹œì‘: {len(base_queries)}ê°œ í‚¤ì›Œë“œ â†’ ê° 1ê°œ ê¸°ì‚¬")

    # 2. ê° í‚¤ì›Œë“œë‹¹ ì œì¼ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ 1ê°œì”© ì„ íƒ
    selected_articles = _fetch_one_per_keyword(base_queries, topic)
    logger.info(f"ì„ íƒëœ ê¸°ì‚¬: {len(selected_articles)}ê°œ")

    # 3. ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì •ë°€ í¬ë¡¤ë§ + AI ë¶„ì„
    full_articles = _crawl_and_analyze(selected_articles, topic=topic)
    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(full_articles)}ê°œ (ì„ íƒ {len(selected_articles)}ê°œ ì¤‘)")

    # í¬ë¡¤ë§ì— ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” Naver ê¸°ë³¸ ì •ë³´(ì œëª©/URL/ì„¤ëª…)ë¡œ í´ë°±
    crawled_urls = {art["url"] for art in full_articles}
    import hashlib
    for art in selected_articles:
        if art["url"] not in crawled_urls:
            logger.info(f"í¬ë¡¤ë§ ì‹¤íŒ¨ í´ë°±: {art['title'][:40]}")
            full_articles.append({
                **art,
                "id": hashlib.md5(art["url"].encode()).hexdigest(),
                "summary_short": art.get("desc", ""),
                "summary": art.get("desc", ""),
                "analysis": {"facts": [], "opinions": []},
                "images": [],
                "charts": [],
            })

    # 4. ê¸°ì‚¬ ì •ë ¬ (ì°¨íŠ¸ ìˆëŠ” ê¸°ì‚¬ ìš°ì„ )
    full_articles.sort(key=lambda x: (len(x.get("charts", [])), len(x.get("images", []))), reverse=True)

    # 5. [Fact Extractor] ê¸°ì‚¬ë³„ í™•ì • ì¸ë±ìŠ¤ë¡œ íŒ©íŠ¸ ìˆ˜ì§‘
    structured_facts = []
    for i, art in enumerate(full_articles):
        art_facts = art.get("analysis", {}).get("facts", [])
        source_name = _extract_source_from_url(art.get("url", "")) or art.get("source", "Unknown")
        article_id = art.get("id", "")
        article_url = art.get("url", "")
        for fact_text in art_facts:
            structured_facts.append({
                "id": f"fact-{uuid.uuid4().hex[:12]}",
                "content": fact_text,
                "source_index": i,
                "source_name": source_name,
                "source_indices": [i],
                "article_id": article_id,
                "article_url": article_url,
                "category": "Fact",
                "visual_proposal": "None",
            })
    logger.info(f"[Fact Extractor] íŒ©íŠ¸ ìˆ˜ì§‘: {len(structured_facts)}ê°œ (ê¸°ì‚¬ {len(full_articles)}ê°œ)")

    # 6. Opinions ëª¨ìŒ
    structured_opinions = []
    for art in full_articles:
        ops = art.get("analysis", {}).get("opinions", [])
        if ops:
            source = art.get("source", "Unknown")
            for op in ops:
                structured_opinions.append(f"[{source}] {op}")

    return {
        "news_data": {
            "articles": full_articles,
            "structured_facts": structured_facts,
            "structured_opinions": structured_opinions,
            "queries_used": base_queries,
            "collected_at": datetime.now().isoformat()
        }
    }


def _search_naver(endpoint: str, keyword: str, headers: dict, display: int = 10) -> List[Dict]:
    """
    Naver ê²€ìƒ‰ API í˜¸ì¶œ ê³µí†µ í•¨ìˆ˜.
    endpoint: "blog", "news", "cafearticle" ë“±
    """
    url = f"https://openapi.naver.com/v1/search/{endpoint}.json"
    try:
        params = {"query": keyword, "display": display, "sort": "sim"}
        res = requests.get(url, headers=headers, params=params, timeout=5)
        if res.status_code != 200:
            logger.warning(f"Naver {endpoint} API ì˜¤ë¥˜ ({keyword}): {res.status_code}")
            return []
        items = res.json().get("items", [])
        results = []
        for item in items:
            link = item.get("originallink") or item.get("link") or item.get("bloggerlink", "")
            if not link:
                continue
            clean_title = re.sub('<[^<]+?>', '', item.get("title", ""))
            clean_desc = re.sub('<[^<]+?>', '', item.get("description", ""))
            results.append({
                "title": clean_title,
                "url": link,
                "desc": clean_desc,
                "pub_date": item.get("pubDate") or item.get("postdate"),
                "query": keyword,
                "source": _extract_source_from_url(link),
                "_search_type": endpoint,
            })
        return results
    except Exception as e:
        logger.warning(f"Naver {endpoint} ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
        return []


def _fetch_one_per_keyword(keywords: List[str], topic: str) -> List[Dict]:
    """
    ê° í‚¤ì›Œë“œë‹¹ ì œì¼ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬/í¬ìŠ¤íŠ¸ 1ê°œì”© ì„ ë³„í•©ë‹ˆë‹¤.

    ê²€ìƒ‰ ìš°ì„ ìˆœìœ„:
      1ìˆœìœ„) Naver Blog  â€” ì‹¤ì‚¬ìš© ë¦¬ë·°, íŠœí† ë¦¬ì–¼, ë¹„êµ ê¸€ (how-to í‚¤ì›Œë“œì— ìµœì )
      2ìˆœìœ„) Naver News  â€” ì–¸ë¡ ì‚¬ ê¸°ì‚¬ (ì—…ê³„ ë™í–¥, í†µê³„, ì‚¬ê±´ ì¤‘ì‹¬)
    ê° ì†ŒìŠ¤ì—ì„œ GPTê°€ "ê´€ë ¨ì—†ìŒ" íŒë‹¨ ì‹œ ë‹¤ìŒ ì†ŒìŠ¤ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
    """
    results = []
    seen_urls: set = set()

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    if not client_id or not client_secret:
        logger.error("NAVER API Key Missing")
        return []

    api_key = os.getenv("OPENAI_API_KEY")
    llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key, temperature=0) if api_key else None

    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}

    for raw_keyword in keywords:
        # ì‰¼í‘œë¡œ ì´ì–´ì§„ ë³µí•© í‚¤ì›Œë“œë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²« ë²ˆì§¸ ìœ íš¨í•œ ê²°ê³¼ ì‚¬ìš©
        sub_keywords = [k.strip() for k in raw_keyword.split(",") if k.strip()]
        if not sub_keywords:
            continue

        found = False
        for keyword in sub_keywords:
            if found:
                break

            # 1ìˆœìœ„: Naver Blog (ì‹¤ì‚¬ìš© ë¦¬ë·°/íŠœí† ë¦¬ì–¼ ì¤‘ì‹¬)
            blog_candidates = [
                c for c in _search_naver("blog", keyword, headers, display=10)
                if c["url"] not in seen_urls
            ]
            if blog_candidates:
                best = _pick_best_article(blog_candidates, keyword, topic, llm)
                if best:
                    seen_urls.add(best["url"])
                    results.append(best)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' [ë¸”ë¡œê·¸]: '{best['title'][:50]}' ì„ íƒ")
                    found = True
                    continue
                else:
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' [ë¸”ë¡œê·¸]: GPT ê´€ë ¨ì—†ìŒ íŒë‹¨ â†’ ë‰´ìŠ¤ë¡œ ì „í™˜")

            # 2ìˆœìœ„: Naver News (ì–¸ë¡ ì‚¬ ê¸°ì‚¬)
            news_candidates = [
                c for c in _search_naver("news", keyword, headers, display=10)
                if c["url"] not in seen_urls
            ]
            if news_candidates:
                best = _pick_best_article(news_candidates, keyword, topic, llm)
                if best:
                    seen_urls.add(best["url"])
                    results.append(best)
                    logger.info(f"í‚¤ì›Œë“œ '{keyword}' [ë‰´ìŠ¤]: '{best['title'][:50]}' ì„ íƒ")
                    found = True
                    continue
                else:
                    logger.warning(f"í‚¤ì›Œë“œ '{keyword}' [ë‰´ìŠ¤]: GPT ê´€ë ¨ì—†ìŒ íŒë‹¨ â†’ ê¸°ì‚¬ ì—†ìŒ ì²˜ë¦¬")

    return results


def _pick_best_article(candidates: List[Dict], keyword: str, topic: str, llm) -> Optional[Dict]:
    """
    ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ì˜ìƒ ì£¼ì œì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê¸€ 1ê°œë¥¼ GPTë¡œ ì„ íƒí•©ë‹ˆë‹¤.
    ëª¨ë“  í›„ë³´ê°€ ê´€ë ¨ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤ (ê±°ë¶€ ê°€ëŠ¥).
    """
    if not candidates:
        return None
    if not llm:
        return candidates[0]

    try:
        article_list = "\n".join(
            f"{i+1}. {art['title']} â€” {art['desc'][:150]}"
            for i, art in enumerate(candidates)
        )

        prompt = f"""ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ ë¦¬ì„œì¹˜ë¥¼ ìœ„í•´ ê°€ì¥ ì í•©í•œ ê¸€ 1ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”.

[ì˜ìƒ ì£¼ì œ]
"{topic}"

[ê²€ìƒ‰ í‚¤ì›Œë“œ]
"{keyword}"

[í›„ë³´ ê¸€]
{article_list}

ì„ íƒ ê¸°ì¤€:
- ê²€ìƒ‰ í‚¤ì›Œë“œ "{keyword}"ì˜ í•µì‹¬ ë‚´ìš©ì„ ì§ì ‘ ë‹¤ë£¨ëŠ” ê¸€
- ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ì— ì¸ìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì‚¬ì‹¤, ì‚¬ë¡€, ì‚¬ìš© ê²½í—˜ì´ ìˆëŠ” ê¸€
- ì‹¤ì‚¬ìš© ë¦¬ë·°, íŠœí† ë¦¬ì–¼, ë¹„êµ ê¸€ ìš°ì„ 
- ê´‘ê³ ì„±Â·í™ë³´ì„± ê¸€ ì œì™¸

âš ï¸ ì¤‘ìš”: í›„ë³´ ê¸€ ì¤‘ í‚¤ì›Œë“œ "{keyword}"ì™€ ì§ì ‘ ê´€ë ¨ëœ ê¸€ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´ ë°˜ë“œì‹œ "0"ì„ ë‹µí•˜ì„¸ìš”.
ì˜ˆë¥¼ ë“¤ì–´ "Copilot ì‚¬ìš©ë²•" í‚¤ì›Œë“œì¸ë° ë„ë¡œê³µì‚¬ë‚˜ ì£¼ì‹ ê¸°ì‚¬ë§Œ ìˆë‹¤ë©´ â†’ 0

ìˆ«ìë§Œ ì‘ë‹µí•˜ì„¸ìš”. ê´€ë ¨ ì—†ìœ¼ë©´ 0, ê´€ë ¨ ìˆìœ¼ë©´ í•´ë‹¹ ë²ˆí˜¸ (ì˜ˆ: 3)"""

        response = llm.invoke(prompt)
        idx_str = response.content.strip()

        # ìˆ«ìë§Œ ì¶”ì¶œ
        digits = re.sub(r'[^\d]', '', idx_str)
        if digits:
            idx = int(digits) - 1
            if idx == -1:
                # GPTê°€ "0" ë°˜í™˜ = ê´€ë ¨ì—†ìŒ ê±°ë¶€
                logger.info(f"ê¸°ì‚¬ ì„ íƒ: GPTê°€ '{keyword}' ê´€ë ¨ ê¸€ ì—†ìŒ íŒë‹¨ â†’ None ë°˜í™˜")
                return None
            if 0 <= idx < len(candidates):
                return candidates[idx]

        return None  # íŒŒì‹± ì‹¤íŒ¨ ì‹œë„ None (ë¶ˆí™•ì‹¤í•œ ê²°ê³¼ í¬í•¨ ì•ˆ í•¨)

    except Exception as e:
        logger.warning(f"ê¸°ì‚¬ ì„ íƒ ì‹¤íŒ¨ ({keyword}): {e} â†’ None ë°˜í™˜")
        return None



# ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ëª… ë§¤í•‘
SOURCE_DOMAIN_MAP = {
    "chosun.com": "ì¡°ì„ ì¼ë³´", "donga.com": "ë™ì•„ì¼ë³´", "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
    "hani.co.kr": "í•œê²¨ë ˆ", "khan.co.kr": "ê²½í–¥ì‹ ë¬¸", "kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
    "seoul.co.kr": "ì„œìš¸ì‹ ë¬¸", "munhwa.com": "ë¬¸í™”ì¼ë³´", "segye.com": "ì„¸ê³„ì¼ë³´",
    "mk.co.kr": "ë§¤ì¼ê²½ì œ", "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´", "hankyung.com": "í•œêµ­ê²½ì œ",
    "sedaily.com": "ì„œìš¸ê²½ì œ", "edaily.co.kr": "ì´ë°ì¼ë¦¬", "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "etnews.com": "ì „ìì‹ ë¬¸", "zdnet.co.kr": "ZDNet Korea",
    "bloter.net": "ë¸”ë¡œí„°", "ddaily.co.kr": "ë””ì§€í„¸ë°ì¼ë¦¬",
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "yonhapnews.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "newsis.com": "ë‰´ì‹œìŠ¤", "news1.kr": "ë‰´ìŠ¤1",
    "bbc.com": "BBC", "bbc.co.uk": "BBC",
    "reuters.com": "Reuters", "bloomberg.com": "Bloomberg",
    "nytimes.com": "NYT", "wsj.com": "WSJ",
    "techcrunch.com": "TechCrunch", "theverge.com": "The Verge",
    "cnbc.com": "CNBC", "ft.com": "FT",
    "fortunekorea.co.kr": "í¬ì¶˜ì½”ë¦¬ì•„", "venturesquare.net": "ë²¤ì²˜ìŠ¤í€˜ì–´",
    "newspim.com": "ë‰´ìŠ¤í•Œ", "theinformation.com": "The Information",
    "inews24.com": "ì•„ì´ë‰´ìŠ¤24", "zdnet.com": "ZDNet",
}

def _extract_source_from_url(url: str) -> str:
    """URL ë„ë©”ì¸ì—ì„œ ì¶œì²˜ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        # ê³µí†µ ì„œë¸Œë„ë©”ì¸ ì œê±°
        for prefix in ("www.", "view.", "news.", "m.", "mobile."):
            if domain.startswith(prefix):
                domain = domain[len(prefix):]

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸/ì¹´í˜ íŠ¹ë³„ ì²˜ë¦¬
        if "blog.naver.com" in domain or "blog.me" in domain:
            return "ë„¤ì´ë²„ ë¸”ë¡œê·¸"
        if "cafe.naver.com" in domain:
            return "ë„¤ì´ë²„ ì¹´í˜"
        if "tistory.com" in domain:
            return "í‹°ìŠ¤í† ë¦¬"
        if "velog.io" in domain:
            return "Velog"
        if "brunch.co.kr" in domain:
            return "ë¸ŒëŸ°ì¹˜"

        # ì •í™•í•œ ë§¤ì¹­
        if domain in SOURCE_DOMAIN_MAP:
            return SOURCE_DOMAIN_MAP[domain]
        # ë¶€ë¶„ ë§¤ì¹­ (ì„œë¸Œë„ë©”ì¸ ëŒ€ì‘)
        for key, name in SOURCE_DOMAIN_MAP.items():
            if key in domain:
                return name
        return ""
    except Exception:
        return ""


import base64
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager

class LegacySSLAdapter(HTTPAdapter):
    """ì˜¤ë˜ëœ ì„œë²„(SSL Legacy Renegotiation) ì ‘ì†ì„ ìœ„í•œ ì–´ëŒ‘í„°"""
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context()
        ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

import uuid

def download_image_to_local(image_url: str, referrer_url: str = None) -> Optional[str]:
    """
    ì´ë¯¸ì§€ë¥¼ public/images/news ê²½ë¡œì— ì €ì¥í•˜ê³  ìƒëŒ€ ê²½ë¡œë¥¼ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    try:
        # BE í´ë” ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ìƒì„± (í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€)
        be_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        save_dir = os.path.join(be_root, "public", "images", "news")
        os.makedirs(save_dir, exist_ok=True)

        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": referrer_url if referrer_url else ""
        }
        
        res = session.get(image_url, headers=headers, timeout=10)
        if res.status_code != 200:
            return None
            
        ext = "jpg"
        if "png" in image_url.lower(): ext = "png"
        if "gif" in image_url.lower(): ext = "gif"
        
        filename = f"{uuid.uuid4()}.{ext}"
        filepath = os.path.join(save_dir, filename)
        
        with open(filepath, "wb") as f:
            f.write(res.content)
            
        return f"/images/news/{filename}"
    except Exception as e:
        logger.error(f"Image Download Failed: {e}")
        return None

import time

def _check_image_context(image_url: str, article_title: str, article_summary: str, referrer_url: str = None) -> Dict:
    """
    GPT-4o-miniì—ê²Œ [ê¸°ì‚¬ ìš”ì•½ + ì´ë¯¸ì§€(Base64)]ë¥¼ ë³´ì—¬ì£¼ê³  íŒë‹¨í•˜ê²Œ í•¨
    (Legacy SSL ì„œë²„ ì§€ì› + Rate Limit ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
    """
    max_retries = 3
    retry_delay = 2  # ì´ˆ
    
    for attempt in range(max_retries):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key: return {"relevant": False}
            
            # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (Legacy SSL ì§€ì› Session ì‚¬ìš©)
            session = requests.Session()
            session.mount('https://', LegacySSLAdapter())
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": referrer_url if referrer_url else ""
            }
            
            img_res = session.get(image_url, headers=headers, timeout=10)
            if img_res.status_code != 200:
                return {"relevant": False}
                
            # 2. Base64 ì¸ì½”ë”©
            encoded_string = base64.b64encode(img_res.content).decode("utf-8")
            data_url = f"data:image/jpeg;base64,{encoded_string}"
            
            llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)
            
            prompt = f"""
            [ë¶„ì„ ìš”ì²­]
            ê¸°ì‚¬ ì œëª©: {article_title}
            ê¸°ì‚¬ ìš”ì•½: {article_summary}
            
            ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì„œ JSONìœ¼ë¡œ ë‹µí•´ì¤˜:
            1. relevant: ì´ ì´ë¯¸ì§€ê°€ ê¸°ì‚¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. type: "chart", "table", "photo", "other"
            3. description: ì´ë¯¸ì§€ ì„¤ëª… (í•œê¸€)
            
            íŒë‹¨ ê¸°ì¤€:
            - chart/table: ê¸°ì‚¬ì˜ ë°ì´í„°/í†µê³„ë¥¼ ì‹œê°í™”í•œ ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ
            - photo: ê¸°ì‚¬ ì£¼ì œì™€ ì§ì ‘ ì—°ê´€ëœ ì‚¬ì§„
              ì˜ˆ) ë¶€ë™ì‚° ê¸°ì‚¬ â†’ ì•„íŒŒíŠ¸/ê±´ë¬¼ ì‚¬ì§„ OK
              ì˜ˆ) ìŠ¤í¬ì¸  ê¸°ì‚¬ â†’ ì„ ìˆ˜/ê²½ê¸° ì‚¬ì§„ OK
              ì˜ˆ) ê²½ì œ ê¸°ì‚¬ â†’ ê´€ë ¨ í˜„ì¥/ì¸ë¬¼ ì‚¬ì§„ OK
            - other: ê´‘ê³ , ë¡œê³ , ë°°ë„ˆ, ì•„ì´ì½˜ â†’ relevant=false
            
            relevant=true ì¡°ê±´:
            - ì°¨íŠ¸/í‘œëŠ” ë¬´ì¡°ê±´ í¬í•¨
            - ì‚¬ì§„ì€ ê¸°ì‚¬ ì£¼ì œì™€ ëª…í™•íˆ ì—°ê´€ëœ ê²½ìš°ë§Œ í¬í•¨
            - ê´‘ê³ /ë¡œê³ /ë°°ë„ˆ/ê¸°ì í”„ë¡œí•„ ì‚¬ì§„ì€ ì œì™¸
            """
            
            msg = HumanMessage(content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_url}}
            ])
            
            res = llm.invoke([msg])
            content = res.content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
            
        except Exception as e:
            if "rate_limit" in str(e).lower() or "429" in str(e):
                if attempt < max_retries - 1:
                    logger.warning(f"Rate limit hit, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    continue
            logger.warning(f"AI Check Error: {e}")
            return {"relevant": False}
    return {"relevant": False}


def _crawl_and_analyze(articles: List[Dict], topic: str = "") -> List[Dict]:
    """Playwrightë¡œ ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ë¥¼ ì‹¹ ê¸ì–´ì˜¤ê³  AIë¡œ ë¶„ì„"""
    results = []
    
    if not articles:
        return []

    def process_one(item):
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # ğŸ­ ë´‡ íƒì§€ ìš°íšŒ: User-Agent ë³€ê²½
                page = browser.new_page(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                
                # ë¡œë”© ëŒ€ê¸°
                try:
                    page.goto(item["url"], timeout=CRAWL_TIMEOUT*1000, wait_until="domcontentloaded")
                    
                    # [Scroll Logic] Lazy Loading ì´ë¯¸ì§€ ë¡œë”©ì„ ìœ„í•´ ìŠ¤í¬ë¡¤ ë‹¤ìš´
                    for _ in range(5):
                        page.evaluate("window.scrollBy(0, document.body.scrollHeight / 5)")
                        page.wait_for_timeout(2000)  # 2.0ì´ˆ ëŒ€ê¸° (Wait longer for lazy loading)
                        
                except:
                    browser.close()
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_html = page.content()
                text = trafilatura.extract(content_html, include_links=False)
                if not text or len(text) < 50:
                    browser.close()
                    return None
                
                # [ID ìƒì„±] URL í•´ì‹œ ê¸°ë°˜ ê³ ìœ  ID ë¶€ì—¬ (Verifier ì—°ê²°ìš©)
                item["id"] = hashlib.md5(item["url"].encode()).hexdigest()

                # [ì¶œì²˜ëª… ìë™ ì¶”ì¶œ] og:site_name ë©”íƒ€íƒœê·¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ê°€ì ¸ì˜¤ê¸°
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(content_html, "html.parser")
                    og_tag = soup.find("meta", property="og:site_name")
                    if og_tag and og_tag.get("content", "").strip():
                        item["og_source"] = og_tag["content"].strip()
                except Exception:
                    pass

                # ì´ë¯¸ì§€ ì¶”ì¶œ (Lazy Loading ì§€ì› + Aggressive Mode)
                # data-src, data-original, data-url ìš°ì„  í™•ì¸
                images_found = []
                
                # [ê°œì„ ] ìŠ¤ë§ˆíŠ¸ ë³¸ë¬¸ ì˜ì—­ ê°ì§€ ì•Œê³ ë¦¬ì¦˜
                def find_article_body_smart(page):
                    """
                    íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ì„ ìë™ ê°ì§€
                    - í…ìŠ¤íŠ¸ ë°€ë„, ë¬¸ë‹¨ ê°œìˆ˜, ì´ë¯¸ì§€ ë“±ì„ ì¢…í•© í‰ê°€
                    """
                    # 1ë‹¨ê³„: ê¸°ì¡´ ì„ íƒì ì‹œë„ (ë¹ ë¥¸ ê²½ë¡œ)
                    common_selectors = [
                        "article", 
                        ".article-body", ".article_body", 
                        "#articleBody", "#newsBody",
                        "div[itemprop='articleBody']",
                    ]
                    
                    for selector in common_selectors:
                        try:
                            elem = page.query_selector(selector)
                            if elem and len(elem.inner_text()) > 200:
                                logger.debug(f"[ARTICLE] ê¸°ì¡´ ì„ íƒìë¡œ ë³¸ë¬¸ ë°œê²¬: {selector}")
                                return elem
                        except:
                            continue
                    
                    # 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ - í›„ë³´ ìš”ì†Œ ìˆ˜ì§‘
                    logger.debug("[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ íƒìƒ‰ ì‹œì‘")
                    candidates = page.query_selector_all('div, section, article')
                    
                    best_score = -999999
                    best_elem = None
                    
                    for elem in candidates:
                        try:
                            text = elem.inner_text()
                            text_len = len(text)
                            
                            # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                            if text_len < 200:
                                continue
                            
                            # ì ìˆ˜ ê³„ì‚° ìš”ì†Œ
                            p_count = len(elem.query_selector_all('p'))
                            li_count = len(elem.query_selector_all('li'))
                            img_count = len(elem.query_selector_all('img'))
                            link_count = len(elem.query_selector_all('a'))
                            
                            # ì ìˆ˜ = í…ìŠ¤íŠ¸ ê¸¸ì´ + ë¬¸ë‹¨ + ì´ë¯¸ì§€ - ë§í¬
                            score = (text_len * 0.3) + ((p_count + li_count) * 100) + (img_count * 50) - (link_count * 10)
                            
                            logger.debug(f"[ARTICLE] í›„ë³´: text={text_len}, p={p_count}, img={img_count}, link={link_count}, score={score:.0f}")
                            
                            if score > best_score:
                                best_score = score
                                best_elem = elem
                        except:
                            continue
                    
                    if best_elem:
                        logger.info(f"[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ë°œê²¬ (ì ìˆ˜: {best_score:.0f})")
                        return best_elem
                    
                    # 3ë‹¨ê³„: í´ë°± - body ì „ì²´
                    logger.warning("[ARTICLE] ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨. body ì „ì²´ ì‚¬ìš©")
                    return page.query_selector('body')
                
                # ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                article_area = find_article_body_smart(page)
                
                # ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì•˜ìœ¼ë©´ ê·¸ ì•ˆì—ì„œë§Œ, ëª» ì°¾ì•˜ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰
                search_area = article_area if article_area else page
                
                # 1. img íƒœê·¸
                imgs = search_area.query_selector_all("img")
                logger.debug(f"[ARTICLE] ê²€ìƒ‰ ì˜ì—­ì—ì„œ ë°œê²¬í•œ img íƒœê·¸: {len(imgs)}ê°œ")
                
                for img in imgs:
                    # Lazy Loading ì†ì„± í™•ì¸
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                        # [Filter] ì“°ë ˆê¸° ì´ë¯¸ì§€ ì œê±° (ê¸°ì, ì•„ì´ì½˜, ë°°ë„ˆ ë“±)
                        src_lower = src.lower()
                        trash_keywords = [
                            '.svg', '.gif', 'logo', 'icon', 'banner', 'ad', 'button', 'btn',
                            'reporter', 'profile', 'journalist', 'avatar'  # ê¸°ì/í”„ë¡œí•„ ì‚¬ì§„ í•„í„° ì¶”ê°€
                        ]
                        if any(x in src_lower for x in trash_keywords):
                            logger.debug(f"[FILTER] í‚¤ì›Œë“œ ì°¨ë‹¨: {src[:80]}")
                            continue
                            
                        # í¬ê¸° ì²´í¬ (JS)
                        try:
                            w = img.evaluate("el => el.naturalWidth")
                            h = img.evaluate("el => el.naturalHeight")
                            
                            # [Filter] í¬ê¸° ê¸°ì¤€ ìƒí–¥ (50px -> 150px)
                            # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì •ë³´ê°€ì¹˜ê°€ ì—†ìŒ
                            if w > 0 and w < 150 and h > 0 and h < 150:
                                logger.debug(f"[FILTER] í¬ê¸° ì°¨ë‹¨: {src[:80]} ({w}x{h})")
                                continue
                                
                            # Lazy Loading ì´ˆê¸°í™” ì „ì´ë¼ 0ì¼ ìˆ˜ë„ ìˆìŒ -> ì¼ë‹¨ URL ë¯¿ê³  ìˆ˜ì§‘ (AIê°€ ìµœì¢… íŒë³„)
                            images_found.append({"url": src, "width": w, "height": h})
                        except:
                            # í¬ê¸° í™•ì¸ ì‹¤íŒ¨í•´ë„ URLì´ ì •ìƒì´ë©´ ì¼ë‹¨ ì¶”ê°€
                            images_found.append({"url": src, "width": 0, "height": 0})
                            
                # 2. figure ì•ˆì˜ ì´ë¯¸ì§€ (ë³´í†µ ì¤‘ìš”í•œ ê¸°ì‚¬ ì´ë¯¸ì§€)
                figures = search_area.query_selector_all("figure img")
                for img in figures:
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                         images_found.append({"url": src, "width": 0, "height": 0})

                # [DEBUG] ì´ë¯¸ì§€ ë°œê²¬ ì§í›„ ë¡œê¹…
                logger.info(f"[DEBUG] {item['url']} - ì›ë³¸ ì´ë¯¸ì§€ ë°œê²¬: {len(images_found)}ê°œ")
                for idx, img in enumerate(images_found[:10]):
                    logger.info(f"  [{idx+1}] {img['url'][:100]}... (w:{img['width']}, h:{img['height']})")
                
                # ì¤‘ë³µ URL ì œê±°
                seen_urls = set()
                candidates = []
                for img in images_found:
                    if img["url"] not in seen_urls:
                        candidates.append(img)
                        seen_urls.add(img["url"])
                
                # [DEBUG] ì¤‘ë³µ ì œê±° í›„ ë¡œê¹…
                logger.info(f"[DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(candidates)}ê°œ")
                
                # --- AI ë¶„ì„ ë‹¨ê³„ (Context check) - ë³‘ë ¬ ì²˜ë¦¬! ---
                final_images = []
                charts = []
                
                # ê¸°ì‚¬ ìš”ì•½ (ì•ë¶€ë¶„ 500ì) - AIì—ê²Œ ë¬¸ë§¥ ì œê³µìš©
                summary = text[:500]
                
                # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ AI ê²€ìˆ˜ (ë¹„ìš© ì¡°ì ˆ)
                target_images = candidates[:5]
                logger.info(f"[DEBUG] AI ë¶„ì„ ì‹œì‘: {len(target_images)}ê°œ ì´ë¯¸ì§€ (ë³‘ë ¬)")
                
                # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
                def analyze_single_image(img):
                    analysis = _check_image_context(img["url"], item["title"], summary, referrer_url=item["url"])
                    if analysis.get("relevant"):
                        img_data = {
                            "url": img["url"],
                            "width": img.get("width", 0),
                            "height": img.get("height", 0),
                            "type": analysis.get("type", "other"),
                            "desc": analysis.get("description", "")
                        }
                        # [Local Save] ì´ë¯¸ì§€ ë¡œì»¬ ì €ì¥
                        local_path = download_image_to_local(img["url"], item["url"])
                        if local_path:
                            img_data["url"] = local_path
                        return (analysis.get("type"), img_data)
                    return None
                
                # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰ (ìµœëŒ€ 5ê°œ ë™ì‹œ)
                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as img_executor:
                    results = list(img_executor.map(analyze_single_image, target_images))
                
                # ê²°ê³¼ ë¶„ë¥˜
                for result in results:
                    if result:
                        img_type, img_data = result
                        if img_type in ["chart", "table"]:
                            charts.append(img_data)
                        else:
                            final_images.append(img_data)
                
                browser.close()
                
                # [AI] ê¸°ì‚¬ ë¶„ì„ ë° UIìš© ë°ì´í„° êµ¬ì¡°í™” (Fact vs Opinion)
                try:
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ 15000ì ì‚¬ìš© (8000 â†’ 15000 í™•ì¥, ê¸´ ê¸°ì‚¬ í›„ë°˜ë¶€ í‚¬ëŸ¬ í¬ì¸íŠ¸ í™•ë³´)
                    input_text = text[:15000] 
                    
                    api_key = os.getenv("OPENAI_API_KEY")
                    if api_key and len(input_text) > 300:
                        llm_extract = ChatOpenAI(model="gpt-5-mini", api_key=api_key, temperature=1)
                        
                        analysis_prompt = f"""ë‹¹ì‹ ì€ YouTube í¬ë¦¬ì—ì´í„°ì˜ ë¦¬ì„œì¹˜ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

[ëª©ì ]
ìœ íŠœë²„ê°€ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ "OOë‰´ìŠ¤ì— ë”°ë¥´ë©´..."ìœ¼ë¡œ ì¸ìš©í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ê°€ëŠ¥í•œ íŒ©íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
ì¶”ì¶œëœ íŒ©íŠ¸ê°€ ìŠ¤í¬ë¦½íŠ¸ì— ë…¹ì•„ë“¤ì–´, ì‹œì²­ìì—ê²Œ "ì´ ì˜ìƒì€ ê·¼ê±° ìˆëŠ” ì •ë³´ë¥¼ ì „ë‹¬í•œë‹¤"ëŠ” ì‹ ë¢°ë¥¼ ì¤ë‹ˆë‹¤.

[ì˜ìƒ ì£¼ì œ]
"{topic}"

âš ï¸ ì ˆëŒ€ ê·œì¹™:
- ê¸°ì‚¬ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆì‹œì˜¤.
- ê´€ë ¨ê¸°ì‚¬ ëª©ë¡ì˜ ì œëª©ì€ ë¶„ì„ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.
- ê¸°ì‚¬ ì „ì²´ë¥¼ ìœ„â†’ì•„ë˜ ìˆœì„œë¡œ ìš”ì•½í•˜ì§€ ë§ˆì‹œì˜¤. ì„ ë³„í•˜ì‹œì˜¤.
- ê°™ì€ ë‚´ìš©ì„ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì‹œì˜¤.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

1. "source": ì–¸ë¡ ì‚¬ëª… (ì˜ˆ: "ë§¤ì¼ê²½ì œ", "TechCrunch")
2. "summary_short": ê¸°ì‚¬ í•µì‹¬ 1ë¬¸ì¥ ìš”ì•½ (í•œêµ­ì–´)
3. "analysis": ì•„ë˜ ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ê°ì²´:

    - "facts": ìœ íŠœë²„ê°€ ìŠ¤í¬ë¦½íŠ¸ì— ì¸ìš©í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤ (í•œêµ­ì–´)
      
      ì¶”ì¶œ ê¸°ì¤€ â€” ì•„ë˜ 4ê°€ì§€ ìœ í˜•ì„ ê°ê° ì°¾ìœ¼ì„¸ìš”:
      
      [ìœ í˜•A: í•µì‹¬ ìˆ˜ì¹˜] ê¸ˆì•¡, ê±´ìˆ˜, ë¹„ìœ¨ ë“± ì„íŒ©íŠ¸ ìˆëŠ” ìˆ«ì (í•µì‹¬ 2~3ê°œë§Œ)
        ì˜ˆ: "ì•¤íŠ¸ë¡œí”½ì€ API ë§¤ì¶œ 31ì–µ ë‹¬ëŸ¬ë¥¼ ë³´ê³ í–ˆë‹¤"
      
      [ìœ í˜•B: ì‚¬ê±´Â·í–‰ìœ„] ëˆ„ê°€ ë¬´ì—‡ì„ í–ˆëŠ”ì§€ â€” ìŠ¤í† ë¦¬ê°€ ë˜ëŠ” ê²ƒ
        ì˜ˆ: "ì•¤íŠ¸ë¡œí”½ì€ ìœ ì••ì‹ ì ˆë‹¨ê¸°ë¡œ ì¤‘ê³ ì±…ì„ ë¶„ë¦¬Â·ìŠ¤ìº”í•´ AI í•™ìŠµì— í™œìš©í–ˆë‹¤"
      
      [ìœ í˜•C: ì§ì ‘ ì¸ìš©] ê¸°ì‚¬ ì† ì¸ë¬¼/ë‹¨ì²´ì˜ ì›ë¬¸ ë°œì–¸ (í°ë”°ì˜´í‘œ ìœ ì§€)
        ì˜ˆ: "ë‹¤ë¦¬ì˜¤ ì•„ëª¨ë°ì´ëŠ” 'ì²˜ìŒ ì•¤íŠ¸ë¡œí”½ì„ ì‹œì‘í–ˆì„ ë•Œ, ì–´ë–»ê²Œ ëˆì„ ë²Œì§€ ì „í˜€ ëª°ëìŠµë‹ˆë‹¤'ë¼ê³  ë§í–ˆë‹¤"
      
      [ìœ í˜•D: ì˜ì™¸ì˜ ë””í…Œì¼] ì‹œì²­ìê°€ ë†€ë„ë§Œí•œ ì—í”¼ì†Œë“œ, ë°˜ì „, ì•„ì´ëŸ¬ë‹ˆ
        ì˜ˆ: "í´ë¡œë“œì—ê²Œ ìíŒê¸°ë¥¼ ìš´ì˜ì‹œì¼°ë”ë‹ˆ, ë¹„ì‹¸ê³  ì“¸ëª¨ì—†ëŠ” í……ìŠ¤í… íë¸Œë¥¼ ì¬ê³ ë¡œ ë“¤ì—¬ë†“ê¸°ë¡œ ê²°ì •í–ˆë‹¤"
      
      âš ï¸ ê¸ˆì§€:
      - ê°™ì€ ì‚¬ì‹¤ì„ ë‹¤ë¥¸ ë¬¸ì¥ìœ¼ë¡œ ë°˜ë³µ (ì¤‘ë³µ ê¸ˆì§€)
      - "ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆë‹¤" ê°™ì€ ëª¨í˜¸í•œ ì„œìˆ 
      - ì˜ìƒ ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ë°°ê²½ ì •ë³´
      - ê¸°ì—…ì´ ìì‚¬ ì œí’ˆ/ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì£¼ì¥í•˜ëŠ” ë‚´ìš© â†’ factsê°€ ì•„ë‹ˆë¼ opinionsì˜ [ì—…ê³„]ë¡œ ë¶„ë¥˜
        ì˜ˆ: "ì•Œë¦¬ë°”ë°”ì— ë”°ë¥´ë©´ 19ê°œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê²½ìŸë ¥ì„ ë³´ì˜€ë‹¤" â†’ [ì—…ê³„]
        ì˜ˆ: "ì‚¼ì„±ì€ ê°¤ëŸ­ì‹œê°€ ì—…ê³„ ìµœê³  ì„±ëŠ¥ì´ë¼ê³  ë°í˜”ë‹¤" â†’ [ì—…ê³„]

    - "opinions": ì „ë¬¸ê°€/ê¸°ê´€ì˜ ì˜ê²¬, í•´ì„, ì „ë§ (í•œêµ­ì–´)
      
      ì¶”ì¶œ ê·œì¹™:
      - factsì— ì´ë¯¸ í¬í•¨ëœ ë‚´ìš©ì„ ë§íˆ¬ë§Œ ë°”ê¿”ì„œ ë„£ì§€ ë§ˆì‹œì˜¤ (ì¤‘ë³µ ê¸ˆì§€)
      - ë°˜ë“œì‹œ ë°œì–¸í•œ ì‚¬ëŒ/ê¸°ê´€ì˜ ì´ë¦„ì´ ìˆì–´ì•¼ í•¨
      - ê¸°ì‚¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë§Œí¼ë§Œ ì¶”ì¶œ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ë„ ê°€ëŠ¥)
      - ì–µì§€ë¡œ ê°œìˆ˜ë¥¼ ì±„ìš°ì§€ ë§ˆì‹œì˜¤
      
      ìœ í˜• íƒœê·¸: [ì „ë¬¸ê°€] [ì—…ê³„] [ì „ë§] [í•´ì„] [ë¶„ì„]
      - [ì „ë¬¸ê°€]: ì´ë¦„+ì§í•¨ì´ ìˆëŠ” ì „ë¬¸ê°€ì˜ ì§ì ‘ ë°œì–¸
      - [ì—…ê³„]: ì—…ê³„ ê´€ê³„ì, í˜‘íšŒ, ê¸°ê´€ì˜ ê³µì‹ ì…ì¥
      - [ì „ë§]: ë¯¸ë˜ ì˜ˆì¸¡ (êµ¬ì²´ì  ê·¼ê±°ê°€ ìˆëŠ” ê²ƒë§Œ)
      - [í•´ì„]: ê¸°ì‚¬ ì† ë¶„ì„ê°€/ì „ë¬¸ê°€ì˜ í•´ì„
      - [ë¶„ì„]: ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì  ì£¼ì¥
      
      [ì¢‹ì€ ì˜ˆì‹œ]
      "[ì „ë¬¸ê°€] ë‹¤ë¦¬ì˜¤ ì•„ëª¨ë°ì´(ì•¤íŠ¸ë¡œí”½ CEO)ëŠ” 'ë°ì´í„°ì„¼í„°ë¥¼ ê·¸ë ‡ê²Œ ë§ì´ ì‚¬ì„œ ìŠ¤ìŠ¤ë¡œë¥¼ ê³¼ë„í•˜ê²Œ ë ˆë²„ë¦¬ì§€í•  ìˆ˜ ìˆì„ê¹Œìš”?'ë¼ê³  ê²½ìŸì‚¬ë¥¼ ë¹„ê¼¬ì•˜ë‹¤"
      "[ì—…ê³„] ì˜êµ­ì¶œíŒí˜‘íšŒëŠ” 'ë¹„ë‚œë°›ì•„ ë§ˆë•…í•˜ë‹¤. ë¹„ë°€ë¡œ ìœ ì§€í•˜ë ¤ í–ˆë‹¤ëŠ” ì‚¬ì‹¤ ìì²´ê°€ ë¬¸ì œì ì„ ì¸ì§€í•˜ê³  ìˆì—ˆìŒì„ ì‹œì‚¬í•œë‹¤'ê³  ì§€ì í–ˆë‹¤"
      
      [ë‚˜ìœ ì˜ˆì‹œ - ì´ë ‡ê²Œ í•˜ì§€ ë§ˆì‹œì˜¤]
      "[ë¶„ì„] AI ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆë‹¤" â† ëª¨í˜¸
      "[ì „ë¬¸ê°€] ì—…ê³„ì—ì„œëŠ” ì„±ì¥í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤" â† ì´ë¦„ ì—†ìŒ
      "[ì „ë§] ë§¤ì¶œì´ 100ì–µ ë‹¬ëŸ¬ì— ê·¼ì ‘í•  ì „ë§ì´ë‹¤" â† factsì— ì´ë¯¸ ìˆëŠ” ë‚´ìš© ì¤‘ë³µ

4. "key_paragraphs": íŒ©íŠ¸/ë°ì´í„°ê°€ í¬í•¨ëœ ì›ë³¸ ë¬¸ë‹¨ ì „ë¶€ (ìˆ˜ì • ì—†ì´ ë³µì‚¬). ì´ì¤‘ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„.

[ê¸°ì‚¬ ë³¸ë¬¸]
{input_text}
"""
                        
                        msg = HumanMessage(content=analysis_prompt)
                        res = llm_extract.invoke([msg])
                        
                        # JSON íŒŒì‹±
                        content = res.content.replace("```json", "").replace("```", "").strip()
                        try:
                            data = json.loads(content)
                            gpt_source = data.get("source", "")
                            # ì¶œì²˜ëª… ê²°ì • ìš°ì„ ìˆœìœ„: URL ë§µ â†’ og:site_name â†’ GPT
                            url_source = _extract_source_from_url(item.get("url", ""))
                            og_source = item.get("og_source", "")
                            if url_source:
                                item["source"] = url_source
                            elif og_source:
                                item["source"] = og_source
                            elif gpt_source and gpt_source not in ("Unknown", "ë¯¸ìƒ", "ì¶œì²˜ë¶ˆëª…", "ì¶œì²˜ ë¯¸ìƒ", "ê¸°ì‚¬(ì¶œì²˜ ë¯¸ìƒ)", "ê¸°ì‚¬(ì œê³µëœ ë³¸ë¬¸)", ""):
                                item["source"] = gpt_source
                            else:
                                item["source"] = og_source or gpt_source or "Unknown"
                            item["summary_short"] = data.get("summary_short", "")
                            item["analysis"] = data.get("analysis", {"facts": [], "opinions": []})
                            
                            # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸(Writer ë“±)ì„ ìœ„í•´ summary í•„ë“œì—ëŠ” ì›ë¬¸ í•µì‹¬ ë¬¸ë‹¨ì„ ìœ ì§€
                            item["summary"] = data.get("key_paragraphs", text[:1000]) 
                            
                            logger.info(f"[AI] ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ: {item['source']} (Facts: {len(item['analysis'].get('facts', []))}, Opinions: {len(item['analysis'].get('opinions', []))})")
                        except json.JSONDecodeError:
                            logger.warning("[AI] JSON íŒŒì‹± ì‹¤íŒ¨, fallback ìˆ˜í–‰")
                            item["source"] = "Unknown"
                            item["summary_short"] = item.get("desc", "")
                            item["analysis"] = {"facts": [], "opinions": []}
                            item["summary"] = text[:1000]
                            
                    else:
                        item["source"] = "Unknown"
                        item["summary_short"] = item.get("desc", "")
                        item["analysis"] = {"facts": [], "opinions": []}
                        item["summary"] = text[:1000] + "..." 

                except Exception as e:
                    logger.warning(f"ê¸°ì‚¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    item["summary"] = text[:1000]
                    item["source"] = "Unknown"
                    item["summary_short"] = item.get("desc", "")
                    item["analysis"] = {"facts": [], "opinions": []}

                item["content"] = text
                item["images"] = final_images
                item["charts"] = charts
                return item
                
        except Exception as e:
            logger.error(f"Crawl failed {item['url']}: {e}")
            return None

    # ë³‘ë ¬ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_one, item): item for item in articles}
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: results.append(res)
            
    return results
