"""
News Research Node (Advanced) - ê³ ì„±ëŠ¥ ë‰´ìŠ¤ ë° ì‹œê° ìë£Œ ìˆ˜ì§‘ê¸°

[í•µì‹¬ ë¡œì§]
1. Deep Fetch: ê²€ìƒ‰ì–´ë‹¹ 15ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í›„ë³´êµ°ì„ ë„“í˜
2. Smart Dedup: ìœ ì‚¬í•œ ì£¼ì œì˜ ê¸°ì‚¬ë¥¼ ê·¸ë£¹í•‘í•˜ê³ , ê° ê·¸ë£¹ì—ì„œ 'ì•Œì§œ ê¸°ì‚¬' 1ê°œì”©ë§Œ ì„ ë³„ (Top 3)
3. Aggressive Crawl: í˜ì´ì§€ ë‚´ 100px ì´ìƒ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘ (ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ ìµœì†Œí™”)
4. AI Context Check: GPT-4o-miniê°€ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ê³¼ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì§„ì§œ ì°¨íŠ¸/í‘œ ë°œêµ´
"""
from typing import Dict, Any, List, Optional
import requests
import json
import trafilatura
import logging
import concurrent.futures
import os
import hashlib
import re
import difflib  # ìœ ì‚¬ë„ ë¹„êµìš©
from datetime import datetime
from playwright.sync_api import sync_playwright
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

# ì„¤ì •
CRAWL_TIMEOUT = 40
MAX_WORKERS = 3 
SIMILARITY_THRESHOLD = 0.6  # ì œëª© ìœ ì‚¬ë„ ê¸°ì¤€ (0.6 ì´ìƒì´ë©´ ê°™ì€ ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼)


def news_research_node(state: Dict[str, Any]) -> Dict[str, Any]:
    logger.info("News Research Node (Advanced) ì‹œì‘")
    
    # 1. ì…ë ¥ ë°ì´í„°
    # Recommenderê°€ ìƒì„±í•œ search_keywordsë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
    channel_profile = state.get("channel_profile", {})
    topic_context = channel_profile.get("topic_context", {})
    base_queries = topic_context.get("search_keywords", []) if topic_context else []
    
    logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ (Recommender): {base_queries}")
    
    if not base_queries:
        return {"news_data": {"articles": []}}
    
    # 2. ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ (Deep Fetch)
    # ì¿¼ë¦¬ë‹¹ 15ê°œì”© ìˆ˜ì§‘ -> í›„ë³´êµ° í™•ë³´
    topic = state.get("topic", "")
    logger.info(f"ë‰´ìŠ¤ í›„ë³´êµ° ìˆ˜ì§‘ ì‹œì‘: ì¿¼ë¦¬ë‹¹ 15ê°œ")
    raw_articles = _fetch_naver_news_bulk(base_queries)
    logger.info(f"ë‰´ìŠ¤ í›„ë³´êµ° í™•ë³´: {len(raw_articles)}ê°œ")
    
    # 3. GPT ê´€ë ¨ë„ í•„í„° (Chain-of-Thought: í•µì‹¬ ëŒ€ìƒ ì¶”ì¶œ â†’ ê´€ë ¨ ê¸°ì‚¬ ì„ ë³„)
    relevant_articles = _filter_relevant_articles(raw_articles, topic, search_keywords=base_queries)
    logger.info(f"ê´€ë ¨ë„ í•„í„° í›„: {len(relevant_articles)}ê°œ (ì›ë³¸ {len(raw_articles)}ê°œ)")
    
    # 4. ì¤‘ë³µ ì œê±° ë° ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • (Smart Dedup)
    # ë¹„ìŠ·í•œ ê¸°ì‚¬ëŠ” ë¬¶ì–´ì„œ ë²„ë¦¬ê³ , ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œì˜ ì•Œì§œ ê¸°ì‚¬ë§Œ ë‚¨ê¹€
    unique_articles = _deduplicate_articles(relevant_articles)
    logger.info(f"ì¤‘ë³µ ì œê±° í›„ ì„ ë³„ëœ Top ê¸°ì‚¬: {len(unique_articles)}ê°œ")
    
    # 5. ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì •ë°€ í¬ë¡¤ë§ (Crawling & AI Analysis)
    # ì„ ë³„ëœ Top ê¸°ì‚¬ë“¤ì— ëŒ€í•´ì„œë§Œ ì •ë°€ ë¶„ì„ ìˆ˜í–‰ (ë¹„ìš© ì ˆê°)
    full_articles = _crawl_and_analyze(unique_articles)
    
    # 6. [Fact Extractor] íŒ©íŠ¸ êµ¬ì¡°í™” ë° ì‹œê°í™” ì œì•ˆ
    # ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ ë¬¸ë‹¨ì„ ë¶„ì„í•˜ì—¬ Writerê°€ ì“°ê¸° í¸í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±
    structured_facts = _structure_facts(full_articles)
    
    # 7. ê²°ê³¼ ë°˜í™˜ (ì°¨íŠ¸ê°€ ìˆëŠ” ê¸°ì‚¬ ìš°ì„  ì •ë ¬)
    full_articles.sort(key=lambda x: (len(x.get("charts", [])), len(x.get("images", []))), reverse=True)
    
    # [NEW] Writerì—ê²Œ ì „ë‹¬í•  Opinions ëª¨ìŒ (ëª¨ë“  ê¸°ì‚¬ì˜ ì˜¤í”¼ë‹ˆì–¸ í†µí•©)
    structured_opinions = []
    for art in full_articles:
        ops = art.get("analysis", {}).get("opinions", [])
        if ops:
            # ì¶œì²˜(Source)ë¥¼ ì•ì— ë¶™ì—¬ì„œ êµ¬ì²´í™” (ì˜ˆ: "[ë§¤ì¼ê²½ì œ] [ì „ë¬¸ê°€] ...")
            source = art.get("source", "Unknown")
            for op in ops:
                structured_opinions.append(f"[{source}] {op}")

    return {
        "news_data": {
            "articles": full_articles,
            "structured_facts": structured_facts, # êµ¬ì¡°í™”ëœ íŒ©íŠ¸ ë¦¬ìŠ¤íŠ¸
            "structured_opinions": structured_opinions, # [NEW] êµ¬ì¡°í™”ëœ ì˜¤í”¼ë‹ˆì–¸ ë¦¬ìŠ¤íŠ¸
            "queries_used": base_queries,
            "collected_at": datetime.now().isoformat()
        }
    }



def _filter_relevant_articles(articles: List[Dict], topic: str, search_keywords: List[str] = None) -> List[Dict]:
    """
    GPT-4o-minië¡œ ê¸°ì‚¬ ì œëª©+ì„¤ëª…ì„ í•œ ë²ˆì— ë³´ë‚´ì„œ
    ì£¼ì œì™€ ê´€ë ¨ ìˆëŠ” ê¸°ì‚¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    
    ë¹„ìš©: ~$0.01 (ì œëª©+ì„¤ëª…ë§Œ ì „ì†¡)
    ì‹œê°„: 2~3ì´ˆ
    """
    if not articles or not topic:
        return articles
    
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY ì—†ìŒ â†’ í•„í„° ìŠ¤í‚µ")
            return articles
        
        # ê¸°ì‚¬ ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„±
        article_list = ""
        for i, art in enumerate(articles):
            article_list += f"{i+1}. [{art.get('title', '')}] {art.get('desc', '')}\n"
        
        keywords_str = ", ".join(search_keywords) if search_keywords else "ì—†ìŒ"
        
        llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key, temperature=0)
        
        prompt = f"""ë‹¹ì‹ ì€ YouTube ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±ì„ ìœ„í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ì„ ë³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì˜ìƒ ì£¼ì œ]
"{topic}"

[ê²€ìƒ‰ í‚¤ì›Œë“œ]
{keywords_str}

[íŒë‹¨ í”„ë¡œì„¸ìŠ¤ - ë°˜ë“œì‹œ ì´ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰]

Step 1. í•µì‹¬ ëŒ€ìƒ ì¶”ì¶œ
ì˜ìƒ ì£¼ì œì—ì„œ í•µì‹¬ ëŒ€ìƒ(ê³ ìœ ëª…ì‚¬: ì œí’ˆëª…, ì¸ë¬¼ëª…, ê¸°ì—…ëª…, ê¸°ìˆ ëª…)ì„ ì¶”ì¶œí•˜ì„¸ìš”.
"AI", "ê¸°ìˆ ", "ì¸ê³µì§€ëŠ¥", "ì‹œì¥", "íŠ¸ë Œë“œ" ê°™ì€ ë²”ìš© ë‹¨ì–´ëŠ” í•µì‹¬ ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.
ì˜ˆ) "ì±—GPTì™€ í´ë¡œë“œ ë¹„êµ ë¶„ì„" â†’ í•µì‹¬ ëŒ€ìƒ: ì±—GPT, ChatGPT, í´ë¡œë“œ, Claude, OpenAI, Anthropic
ì˜ˆ) "ì—”ë¹„ë””ì•„ ì£¼ê°€ ì „ë§" â†’ í•µì‹¬ ëŒ€ìƒ: ì—”ë¹„ë””ì•„, NVIDIA, ì  ìŠ¨í™©

Step 2. ê¸°ì‚¬ë³„ íŒë‹¨
ê° ê¸°ì‚¬ì— ëŒ€í•´ ì´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:
"ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì“°ëŠ” ì‚¬ëŒì´ ì´ ê¸°ì‚¬ë¥¼ ì—´ì—ˆì„ ë•Œ, ìŠ¤í¬ë¦½íŠ¸ì— ì§ì ‘ ì¸ìš©í•  ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ìˆëŠ”ê°€?"

í¬í•¨ (O):
- ê¸°ì‚¬ì˜ ì£¼ì œ ìì²´ê°€ í•µì‹¬ ëŒ€ìƒì— ê´€í•œ ê²ƒ
- í•µì‹¬ ëŒ€ìƒì˜ ì„±ëŠ¥, ê¸°ëŠ¥, ì—…ë°ì´íŠ¸, ë¹„êµë¥¼ ì§ì ‘ ë‹¤ë£¨ëŠ” ê¸°ì‚¬
- í•µì‹¬ ëŒ€ìƒì— ëŒ€í•œ ì „ë¬¸ê°€ ì˜ê²¬, ë°ì´í„°, ë¶„ì„ì´ ë‹´ê¸´ ê¸°ì‚¬

ì œì™¸ (X) - ì•„ë˜ í•˜ë‚˜ë¼ë„ í•´ë‹¹í•˜ë©´ ë¬´ì¡°ê±´ ì œì™¸:
- í•µì‹¬ ëŒ€ìƒì´ ì œëª©ê³¼ ì„¤ëª…ì— ì „í˜€ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ê¸°ì‚¬
- ë‹¤ë¥¸ ë¶„ì•¼(ì˜í•™, êµìœ¡, ë¶€ë™ì‚°, ê¸°ì—…ì „ëµ ë“±) ê¸°ì‚¬ì—ì„œ í•µì‹¬ ëŒ€ìƒì„ ë„êµ¬/ì‚¬ë¡€ë¡œ ì ê¹ ì–¸ê¸‰í•˜ëŠ” ê¸°ì‚¬
- ê¸°ì—… ì „ëµ, ì£¼ì‹, íˆ¬ì ê¸°ì‚¬ì—ì„œ í•µì‹¬ ëŒ€ìƒ ì´ë¦„ì´ ë‚˜ì˜¬ ë¿ì¸ ê¸°ì‚¬

âš ï¸ ì œëª©ì— í•µì‹¬ ëŒ€ìƒì´ ì—†ê³  ì„¤ëª…ì—ë§Œ ìˆëŠ” ê¸°ì‚¬ëŠ” íŠ¹íˆ ì˜ì‹¬í•˜ì„¸ìš”.
ë‹¤ë¥¸ ë¶„ì•¼ ê¸°ì‚¬ì—ì„œ ì ê¹ ì–¸ê¸‰í•˜ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì…ë‹ˆë‹¤.

[ê¸°ì‚¬ ëª©ë¡]
{article_list}

[ì‘ë‹µ í˜•ì‹ - ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ]
í•µì‹¬ëŒ€ìƒ: (ì¶”ì¶œí•œ í•µì‹¬ ëŒ€ìƒ ë‚˜ì—´)
ê´€ë ¨ê¸°ì‚¬: (ë²ˆí˜¸ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: 1,3,5)

ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´:
í•µì‹¬ëŒ€ìƒ: (ì¶”ì¶œí•œ í•µì‹¬ ëŒ€ìƒ ë‚˜ì—´)
ê´€ë ¨ê¸°ì‚¬: ì—†ìŒ"""
        
        response = llm.invoke(prompt)
        content = response.content.strip()
        
        # Chain-of-Thought ì‘ë‹µ íŒŒì‹±
        lines = content.strip().split("\n")
        core_entities_line = ""
        articles_line = ""
        
        for line in lines:
            line_stripped = line.strip()
            if line_stripped.startswith("í•µì‹¬ëŒ€ìƒ:"):
                core_entities_line = line_stripped.replace("í•µì‹¬ëŒ€ìƒ:", "").strip()
            elif line_stripped.startswith("ê´€ë ¨ê¸°ì‚¬:"):
                articles_line = line_stripped.replace("ê´€ë ¨ê¸°ì‚¬:", "").strip()
        
        logger.info(f"GPT í•µì‹¬ëŒ€ìƒ: {core_entities_line}")
        logger.info(f"GPT ê´€ë ¨ê¸°ì‚¬: {articles_line}")
        
        if articles_line == "ì—†ìŒ" or not articles_line:
            logger.warning("GPTê°€ ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒìœ¼ë¡œ íŒë‹¨ â†’ ì›ë³¸ ìœ ì§€")
            return articles
        
        # ë²ˆí˜¸ íŒŒì‹±
        try:
            relevant_indices = [int(x.strip()) - 1 for x in articles_line.split(",") if x.strip().isdigit()]
            filtered = [articles[i] for i in relevant_indices if 0 <= i < len(articles)]
            
            if not filtered:
                logger.warning("í•„í„° ê²°ê³¼ 0ê°œ â†’ ì›ë³¸ ìœ ì§€")
                return articles
            
            return filtered
        except Exception as e:
            logger.warning(f"GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e} â†’ ì›ë³¸ ìœ ì§€")
            return articles
            
    except Exception as e:
        logger.warning(f"ê´€ë ¨ë„ í•„í„° ì—ëŸ¬: {e} â†’ í•„í„° ìŠ¤í‚µ")
        return articles


def _fetch_naver_news_bulk(queries: List[str]) -> List[Dict]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ê²°ê³¼ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì¿¼ë¦¬ë‹¹ 15ê°œ)."""
    articles = []
    seen_links = set()
    
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    
    if not client_id or not client_secret:
        logger.error("NAVER API Key Missing")
        return []

    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    url = "https://openapi.naver.com/v1/search/news.json"
    
    for query in queries:
        try:
            # 15ê°œ ìˆ˜ì§‘ (display=15)
            params = {"query": query, "display": 15, "sort": "sim"}
            res = requests.get(url, headers=headers, params=params, timeout=5)
            
            if res.status_code == 200:
                items = res.json().get("items", [])
                for item in items:
                    link = item.get("originallink") or item.get("link")
                    if link and link not in seen_links:
                        clean_title = re.sub('<[^<]+?>', '', item.get("title", ""))
                        clean_desc = re.sub('<[^<]+?>', '', item.get("description", ""))
                        
                        articles.append({
                            "title": clean_title,
                            "url": link,
                            "desc": clean_desc,
                            "pub_date": item.get("pubDate"),
                            "query": query
                        })
                        seen_links.add(link)
        except Exception as e:
            logger.warning(f"Naver Search Error ({query}): {e}")
            
    return articles


def _deduplicate_articles(articles: List[Dict]) -> List[Dict]:
    """
    ìœ ì‚¬í•œ ì œëª©ì„ ê°€ì§„ ê¸°ì‚¬ë“¤ì„ ê·¸ë£¹í™”í•˜ê³ , 
    ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ì˜ì–‘ê°€ ìˆëŠ”(ì„¤ëª…ì´ ê¸¸ê±°ë‚˜ í‚¤ì›Œë“œê°€ ìˆëŠ”) ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì”© ë½‘ì•„ëƒ…ë‹ˆë‹¤.
    ìµœì¢…ì ìœ¼ë¡œ Top 3~5ê°œë§Œ ë¦¬í„´í•©ë‹ˆë‹¤.
    """
    if not articles:
        return []
        
    clusters = []
    visited = [False] * len(articles)
    
    # 1. Clustering (ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í•‘)
    for i in range(len(articles)):
        if visited[i]:
            continue
            
        current_cluster = [articles[i]]
        visited[i] = True
        
        for j in range(i + 1, len(articles)):
            if visited[j]:
                continue
                
            # ë¬¸ì¥ ìœ ì‚¬ë„ ë¹„êµ (ì œëª© OR ë‚´ìš© OR í‚¤ì›Œë“œ) - ì…‹ ì¤‘ í•˜ë‚˜ë¼ë„ ë†’ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            title_sim = difflib.SequenceMatcher(None, articles[i]["title"], articles[j]["title"]).ratio()
            desc_sim = difflib.SequenceMatcher(None, articles[i]["desc"], articles[j]["desc"]).ratio()
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ (Jaccard Similarity)
            # ì œëª©ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
            words_i = set(articles[i]["title"].split())
            words_j = set(articles[j]["title"].split())
            intersection = len(words_i & words_j)
            union = len(words_i | words_j)
            keyword_sim = intersection / union if union > 0 else 0
            
            # ì œëª© 40% ì´ìƒ OR ë‚´ìš© 70% ì´ìƒ OR í‚¤ì›Œë“œ 50% ì´ìƒ ë¹„ìŠ·í•˜ë©´ ê°™ì€ ê¸°ì‚¬
            if title_sim >= 0.4 or desc_sim >= 0.7 or keyword_sim >= 0.5:
                current_cluster.append(articles[j])
                visited[j] = True
        
        clusters.append(current_cluster)
    
    # 2. Representative Selection (ëŒ€í‘œ ê¸°ì‚¬ ì„ ì •)
    final_articles = []
    for cluster in clusters:
        # ì ìˆ˜ ê³„ì‚°: ì„¤ëª… ê¸¸ì´ + ('í‘œ'/'ê·¸ë˜í”„' í‚¤ì›Œë“œ ê°€ì‚°ì )
        best_article = cluster[0]
        max_score = -1
        
        for art in cluster:
            score = len(art["desc"])  # ê¸°ë³¸ ì ìˆ˜: ì„¤ëª…ì´ ìì„¸í• ìˆ˜ë¡ ì¢‹ìŒ
            
            # ê°€ì‚°ì : í‘œ/ê·¸ë˜í”„/ì¢…í•©/ë¶„ì„ ê°™ì€ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ë°ì´í„°ê°€ ë§ì„ í™•ë¥  ë†’ìŒ
            keywords = ["í‘œ", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ì¶”ì´", "í˜„í™©", "ì¢…í•©", "ë¶„ì„"]
            if any(k in art["title"] for k in keywords):
                score += 200
            if any(k in art["desc"] for k in keywords):
                score += 100
            
            if score > max_score:
                max_score = score
                best_article = art
                
        final_articles.append(best_article)
        
    # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë°˜í™˜ (ë‹¤ì–‘ì„± í™•ë³´ëœ ìƒíƒœ)
    return final_articles[:5]



import base64
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager

class LegacySSLAdapter(HTTPAdapter):
    """ì˜¤ë˜ëœ ì„œë²„(SSL Legacy Renegotiation) ì ‘ì†ì„ ìœ„í•œ ì–´ëŒ‘í„°"""
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context()
        ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

import uuid

def download_image_to_local(image_url: str, referrer_url: str = None) -> Optional[str]:
    """
    ì´ë¯¸ì§€ë¥¼ public/images/news ê²½ë¡œì— ì €ì¥í•˜ê³  ìƒëŒ€ ê²½ë¡œë¥¼ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    try:
        # BE í´ë” ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ìƒì„± (í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€)
        be_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        save_dir = os.path.join(be_root, "public", "images", "news")
        os.makedirs(save_dir, exist_ok=True)

        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": referrer_url if referrer_url else ""
        }
        
        res = session.get(image_url, headers=headers, timeout=10)
        if res.status_code != 200:
            return None
            
        ext = "jpg"
        if "png" in image_url.lower(): ext = "png"
        if "gif" in image_url.lower(): ext = "gif"
        
        filename = f"{uuid.uuid4()}.{ext}"
        filepath = os.path.join(save_dir, filename)
        
        with open(filepath, "wb") as f:
            f.write(res.content)
            
        return f"/images/news/{filename}"
    except Exception as e:
        logger.error(f"Image Download Failed: {e}")
        return None

import time

def _check_image_context(image_url: str, article_title: str, article_summary: str, referrer_url: str = None) -> Dict:
    """
    GPT-4o-miniì—ê²Œ [ê¸°ì‚¬ ìš”ì•½ + ì´ë¯¸ì§€(Base64)]ë¥¼ ë³´ì—¬ì£¼ê³  íŒë‹¨í•˜ê²Œ í•¨
    (Legacy SSL ì„œë²„ ì§€ì› + Rate Limit ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
    """
    max_retries = 3
    retry_delay = 2  # ì´ˆ
    
    for attempt in range(max_retries):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key: return {"relevant": False}
            
            # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (Legacy SSL ì§€ì› Session ì‚¬ìš©)
            session = requests.Session()
            session.mount('https://', LegacySSLAdapter())
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": referrer_url if referrer_url else ""
            }
            
            img_res = session.get(image_url, headers=headers, timeout=10)
            if img_res.status_code != 200:
                return {"relevant": False}
                
            # 2. Base64 ì¸ì½”ë”©
            encoded_string = base64.b64encode(img_res.content).decode("utf-8")
            data_url = f"data:image/jpeg;base64,{encoded_string}"
            
            llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)
            
            prompt = f"""
            [ë¶„ì„ ìš”ì²­]
            ê¸°ì‚¬ ì œëª©: {article_title}
            ê¸°ì‚¬ ìš”ì•½: {article_summary}
            
            ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì„œ JSONìœ¼ë¡œ ë‹µí•´ì¤˜:
            1. relevant: ì´ ì´ë¯¸ì§€ê°€ ê¸°ì‚¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. type: "chart", "table", "photo", "other"
            3. description: ì´ë¯¸ì§€ ì„¤ëª… (í•œê¸€)
            
            íŒë‹¨ ê¸°ì¤€:
            - chart/table: ê¸°ì‚¬ì˜ ë°ì´í„°/í†µê³„ë¥¼ ì‹œê°í™”í•œ ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ
            - photo: ê¸°ì‚¬ ì£¼ì œì™€ ì§ì ‘ ì—°ê´€ëœ ì‚¬ì§„
              ì˜ˆ) ë¶€ë™ì‚° ê¸°ì‚¬ â†’ ì•„íŒŒíŠ¸/ê±´ë¬¼ ì‚¬ì§„ OK
              ì˜ˆ) ìŠ¤í¬ì¸  ê¸°ì‚¬ â†’ ì„ ìˆ˜/ê²½ê¸° ì‚¬ì§„ OK
              ì˜ˆ) ê²½ì œ ê¸°ì‚¬ â†’ ê´€ë ¨ í˜„ì¥/ì¸ë¬¼ ì‚¬ì§„ OK
            - other: ê´‘ê³ , ë¡œê³ , ë°°ë„ˆ, ì•„ì´ì½˜ â†’ relevant=false
            
            relevant=true ì¡°ê±´:
            - ì°¨íŠ¸/í‘œëŠ” ë¬´ì¡°ê±´ í¬í•¨
            - ì‚¬ì§„ì€ ê¸°ì‚¬ ì£¼ì œì™€ ëª…í™•íˆ ì—°ê´€ëœ ê²½ìš°ë§Œ í¬í•¨
            - ê´‘ê³ /ë¡œê³ /ë°°ë„ˆ/ê¸°ì í”„ë¡œí•„ ì‚¬ì§„ì€ ì œì™¸
            """
            
            msg = HumanMessage(content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_url}}
            ])
            
            res = llm.invoke([msg])
            content = res.content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
            
        except Exception as e:
            if "rate_limit" in str(e).lower() or "429" in str(e):
                if attempt < max_retries - 1:
                    logger.warning(f"Rate limit hit, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    continue
            logger.warning(f"AI Check Error: {e}")
            return {"relevant": False}
    return {"relevant": False}


def _crawl_and_analyze(articles: List[Dict]) -> List[Dict]:
    """Playwrightë¡œ ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ë¥¼ ì‹¹ ê¸ì–´ì˜¤ê³  AIë¡œ ë¶„ì„"""
    results = []
    
    if not articles:
        return []

    def process_one(item):
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # ğŸ­ ë´‡ íƒì§€ ìš°íšŒ: User-Agent ë³€ê²½
                page = browser.new_page(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                
                # ë¡œë”© ëŒ€ê¸°
                try:
                    page.goto(item["url"], timeout=CRAWL_TIMEOUT*1000, wait_until="domcontentloaded")
                    
                    # [Scroll Logic] Lazy Loading ì´ë¯¸ì§€ ë¡œë”©ì„ ìœ„í•´ ìŠ¤í¬ë¡¤ ë‹¤ìš´
                    for _ in range(5):
                        page.evaluate("window.scrollBy(0, document.body.scrollHeight / 5)")
                        page.wait_for_timeout(2000)  # 2.0ì´ˆ ëŒ€ê¸° (Wait longer for lazy loading)
                        
                except:
                    browser.close()
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_html = page.content()
                text = trafilatura.extract(content_html, include_links=False)
                if not text or len(text) < 50:
                    browser.close()
                    return None
                
                # [ID ìƒì„±] URL í•´ì‹œ ê¸°ë°˜ ê³ ìœ  ID ë¶€ì—¬ (Verifier ì—°ê²°ìš©)
                item["id"] = hashlib.md5(item["url"].encode()).hexdigest()

                # ì´ë¯¸ì§€ ì¶”ì¶œ (Lazy Loading ì§€ì› + Aggressive Mode)
                # data-src, data-original, data-url ìš°ì„  í™•ì¸
                images_found = []
                
                # [ê°œì„ ] ìŠ¤ë§ˆíŠ¸ ë³¸ë¬¸ ì˜ì—­ ê°ì§€ ì•Œê³ ë¦¬ì¦˜
                def find_article_body_smart(page):
                    """
                    íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ì„ ìë™ ê°ì§€
                    - í…ìŠ¤íŠ¸ ë°€ë„, ë¬¸ë‹¨ ê°œìˆ˜, ì´ë¯¸ì§€ ë“±ì„ ì¢…í•© í‰ê°€
                    """
                    # 1ë‹¨ê³„: ê¸°ì¡´ ì„ íƒì ì‹œë„ (ë¹ ë¥¸ ê²½ë¡œ)
                    common_selectors = [
                        "article", 
                        ".article-body", ".article_body", 
                        "#articleBody", "#newsBody",
                        "div[itemprop='articleBody']",
                    ]
                    
                    for selector in common_selectors:
                        try:
                            elem = page.query_selector(selector)
                            if elem and len(elem.inner_text()) > 200:
                                logger.debug(f"[ARTICLE] ê¸°ì¡´ ì„ íƒìë¡œ ë³¸ë¬¸ ë°œê²¬: {selector}")
                                return elem
                        except:
                            continue
                    
                    # 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ - í›„ë³´ ìš”ì†Œ ìˆ˜ì§‘
                    logger.debug("[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ íƒìƒ‰ ì‹œì‘")
                    candidates = page.query_selector_all('div, section, article')
                    
                    best_score = -999999
                    best_elem = None
                    
                    for elem in candidates:
                        try:
                            text = elem.inner_text()
                            text_len = len(text)
                            
                            # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                            if text_len < 200:
                                continue
                            
                            # ì ìˆ˜ ê³„ì‚° ìš”ì†Œ
                            p_count = len(elem.query_selector_all('p'))
                            li_count = len(elem.query_selector_all('li'))
                            img_count = len(elem.query_selector_all('img'))
                            link_count = len(elem.query_selector_all('a'))
                            
                            # ì ìˆ˜ = í…ìŠ¤íŠ¸ ê¸¸ì´ + ë¬¸ë‹¨ + ì´ë¯¸ì§€ - ë§í¬
                            score = (text_len * 0.3) + ((p_count + li_count) * 100) + (img_count * 50) - (link_count * 10)
                            
                            logger.debug(f"[ARTICLE] í›„ë³´: text={text_len}, p={p_count}, img={img_count}, link={link_count}, score={score:.0f}")
                            
                            if score > best_score:
                                best_score = score
                                best_elem = elem
                        except:
                            continue
                    
                    if best_elem:
                        logger.info(f"[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ë°œê²¬ (ì ìˆ˜: {best_score:.0f})")
                        return best_elem
                    
                    # 3ë‹¨ê³„: í´ë°± - body ì „ì²´
                    logger.warning("[ARTICLE] ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨. body ì „ì²´ ì‚¬ìš©")
                    return page.query_selector('body')
                
                # ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                article_area = find_article_body_smart(page)
                
                # ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì•˜ìœ¼ë©´ ê·¸ ì•ˆì—ì„œë§Œ, ëª» ì°¾ì•˜ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰
                search_area = article_area if article_area else page
                
                # 1. img íƒœê·¸
                imgs = search_area.query_selector_all("img")
                logger.debug(f"[ARTICLE] ê²€ìƒ‰ ì˜ì—­ì—ì„œ ë°œê²¬í•œ img íƒœê·¸: {len(imgs)}ê°œ")
                
                for img in imgs:
                    # Lazy Loading ì†ì„± í™•ì¸
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                        # [Filter] ì“°ë ˆê¸° ì´ë¯¸ì§€ ì œê±° (ê¸°ì, ì•„ì´ì½˜, ë°°ë„ˆ ë“±)
                        src_lower = src.lower()
                        trash_keywords = [
                            '.svg', '.gif', 'logo', 'icon', 'banner', 'ad', 'button', 'btn',
                            'reporter', 'profile', 'journalist', 'avatar'  # ê¸°ì/í”„ë¡œí•„ ì‚¬ì§„ í•„í„° ì¶”ê°€
                        ]
                        if any(x in src_lower for x in trash_keywords):
                            logger.debug(f"[FILTER] í‚¤ì›Œë“œ ì°¨ë‹¨: {src[:80]}")
                            continue
                            
                        # í¬ê¸° ì²´í¬ (JS)
                        try:
                            w = img.evaluate("el => el.naturalWidth")
                            h = img.evaluate("el => el.naturalHeight")
                            
                            # [Filter] í¬ê¸° ê¸°ì¤€ ìƒí–¥ (50px -> 150px)
                            # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì •ë³´ê°€ì¹˜ê°€ ì—†ìŒ
                            if w > 0 and w < 150 and h > 0 and h < 150:
                                logger.debug(f"[FILTER] í¬ê¸° ì°¨ë‹¨: {src[:80]} ({w}x{h})")
                                continue
                                
                            # Lazy Loading ì´ˆê¸°í™” ì „ì´ë¼ 0ì¼ ìˆ˜ë„ ìˆìŒ -> ì¼ë‹¨ URL ë¯¿ê³  ìˆ˜ì§‘ (AIê°€ ìµœì¢… íŒë³„)
                            images_found.append({"url": src, "width": w, "height": h})
                        except:
                            # í¬ê¸° í™•ì¸ ì‹¤íŒ¨í•´ë„ URLì´ ì •ìƒì´ë©´ ì¼ë‹¨ ì¶”ê°€
                            images_found.append({"url": src, "width": 0, "height": 0})
                            
                # 2. figure ì•ˆì˜ ì´ë¯¸ì§€ (ë³´í†µ ì¤‘ìš”í•œ ê¸°ì‚¬ ì´ë¯¸ì§€)
                figures = search_area.query_selector_all("figure img")
                for img in figures:
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                         images_found.append({"url": src, "width": 0, "height": 0})

                # [DEBUG] ì´ë¯¸ì§€ ë°œê²¬ ì§í›„ ë¡œê¹…
                logger.info(f"[DEBUG] {item['url']} - ì›ë³¸ ì´ë¯¸ì§€ ë°œê²¬: {len(images_found)}ê°œ")
                for idx, img in enumerate(images_found[:10]):
                    logger.info(f"  [{idx+1}] {img['url'][:100]}... (w:{img['width']}, h:{img['height']})")
                
                # ì¤‘ë³µ URL ì œê±°
                seen_urls = set()
                candidates = []
                for img in images_found:
                    if img["url"] not in seen_urls:
                        candidates.append(img)
                        seen_urls.add(img["url"])
                
                # [DEBUG] ì¤‘ë³µ ì œê±° í›„ ë¡œê¹…
                logger.info(f"[DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(candidates)}ê°œ")
                
                # --- AI ë¶„ì„ ë‹¨ê³„ (Context check) - ë³‘ë ¬ ì²˜ë¦¬! ---
                final_images = []
                charts = []
                
                # ê¸°ì‚¬ ìš”ì•½ (ì•ë¶€ë¶„ 500ì) - AIì—ê²Œ ë¬¸ë§¥ ì œê³µìš©
                summary = text[:500]
                
                # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ AI ê²€ìˆ˜ (ë¹„ìš© ì¡°ì ˆ)
                target_images = candidates[:5]
                logger.info(f"[DEBUG] AI ë¶„ì„ ì‹œì‘: {len(target_images)}ê°œ ì´ë¯¸ì§€ (ë³‘ë ¬)")
                
                # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
                def analyze_single_image(img):
                    analysis = _check_image_context(img["url"], item["title"], summary, referrer_url=item["url"])
                    if analysis.get("relevant"):
                        img_data = {
                            "url": img["url"],
                            "width": img.get("width", 0),
                            "height": img.get("height", 0),
                            "type": analysis.get("type", "other"),
                            "desc": analysis.get("description", "")
                        }
                        # [Local Save] ì´ë¯¸ì§€ ë¡œì»¬ ì €ì¥
                        local_path = download_image_to_local(img["url"], item["url"])
                        if local_path:
                            img_data["url"] = local_path
                        return (analysis.get("type"), img_data)
                    return None
                
                # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì‹¤í–‰ (ìµœëŒ€ 5ê°œ ë™ì‹œ)
                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as img_executor:
                    results = list(img_executor.map(analyze_single_image, target_images))
                
                # ê²°ê³¼ ë¶„ë¥˜
                for result in results:
                    if result:
                        img_type, img_data = result
                        if img_type in ["chart", "table"]:
                            charts.append(img_data)
                        else:
                            final_images.append(img_data)
                
                browser.close()
                
                # [AI] ê¸°ì‚¬ ë¶„ì„ ë° UIìš© ë°ì´í„° êµ¬ì¡°í™” (Fact vs Opinion)
                try:
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ 4000ìë§Œ ì‚¬ìš©
                    input_text = text[:4000] 
                    
                    api_key = os.getenv("OPENAI_API_KEY")
                    if api_key and len(input_text) > 300:
                        llm_extract = ChatOpenAI(model="gpt-4o-mini", api_key=api_key, temperature=0.0)
                        
                        analysis_prompt = f"""
                        Analyze the news article below and return a JSON object with the following fields:

                        1. "source": Name of the news outlet/press (e.g., "TechCrunch", "ë§¤ì¼ê²½ì œ"). Extract from text or infer from context.
                        2. "summary_short": A single sentence summary of the most critical point (Korean).
                        3. "analysis": An object containing two lists:
                            - "facts": A list of objective facts, statistics, specific numbers, and confirmed events (Korean).
                            - "opinions": A list of subjective claims, interpretations, and predictions (Korean). 
                                          **CRITICAL EXECUTION & SORTING ORDER (Final Logic)**:
                                          
                                          1. **Extraction (UNLIMITED)**:
                                             - Extract ALL explicit quotes/opinions from specific speakers (`[ì „ë¬¸ê°€]`, `[ì—…ê³„]`, `[ë¶„ì„]`).
                                             - Extract ALL valid `[ì „ë§]` (Outlook) and `[í•´ì„]` (Insight) from the text.
                                          
                                          2. **Slot Filling Strategy (Target: 5 Items)**:
                                             - **Slot 1 (Mandatory)**: Best `[ì „ë§]`
                                             - **Slot 2 (Mandatory)**: Best `[í•´ì„]`
                                             - **Slot 3 (Priority)**: `[ì „ë¬¸ê°€]` if exists. Else -> Backfill with extra `[ì „ë§]`.
                                             - **Slot 4 (Priority)**: `[ì—…ê³„]` if exists. Else -> Backfill with extra `[í•´ì„]`.
                                             - **Slot 5 (Priority)**: `[ë¶„ì„]` if exists. Else -> Backfill with extra `[ì „ë§]` or `[í•´ì„]`.
                                          
                                          3. **Final Grouping (Strict Sorting)**:
                                             Regardless of how slots were filled, you MUST sort the final list in this grouped order:
                                             
                                             **GROUP 1**: All `[ì „ë§]` items (Original + Backfilled)
                                             **GROUP 2**: All `[í•´ì„]` items (Original + Backfilled)
                                             **GROUP 3**: All `[ì „ë¬¸ê°€]` items
                                             **GROUP 4**: All `[ì—…ê³„]` items
                                             **GROUP 5**: All `[ë¶„ì„]` items
                                          
                                          Example Output (Grouped):
                                          "[ì „ë§] 2026ë…„ ì„±ì¥ì„¸ ì§€ì†", "[ì „ë§] (ì¶”ê°€) ì•„ì‹œì•„ ì‹œì¥ í™•ëŒ€", "[í•´ì„] ê·œì œ ì™„í™” ë•ë¶„", "[ì „ë¬¸ê°€] ê¹€êµìˆ˜ëŠ”...", "[ì—…ê³„] ì´ëŒ€í‘œëŠ”..."
                        4. "key_paragraphs": Extract ALL original distinct paragraphs that contain facts/data (No rewriting, just copy-paste). Separated by double newlines.

                        [Article Text]
                        {input_text}
                        """
                        
                        msg = HumanMessage(content=analysis_prompt)
                        res = llm_extract.invoke([msg])
                        
                        # JSON íŒŒì‹±
                        content = res.content.replace("```json", "").replace("```", "").strip()
                        try:
                            data = json.loads(content)
                            item["source"] = data.get("source", "Unknown")
                            item["summary_short"] = data.get("summary_short", "")
                            item["analysis"] = data.get("analysis", {"facts": [], "opinions": []})
                            
                            # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸(Writer ë“±)ì„ ìœ„í•´ summary í•„ë“œì—ëŠ” ì›ë¬¸ í•µì‹¬ ë¬¸ë‹¨ì„ ìœ ì§€
                            item["summary"] = data.get("key_paragraphs", text[:1000]) 
                            
                            logger.info(f"[AI] ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ: {item['source']} (Facts: {len(item['analysis']['facts'])}, Opinions: {len(item['analysis']['opinions'])})")
                        except json.JSONDecodeError:
                            logger.warning("[AI] JSON íŒŒì‹± ì‹¤íŒ¨, fallback ìˆ˜í–‰")
                            item["source"] = "Unknown"
                            item["summary_short"] = item.get("desc", "")
                            item["analysis"] = {"facts": [], "opinions": []}
                            item["summary"] = text[:1000]
                            
                    else:
                        item["source"] = "Unknown"
                        item["summary_short"] = item.get("desc", "")
                        item["analysis"] = {"facts": [], "opinions": []}
                        item["summary"] = text[:1000] + "..." 

                except Exception as e:
                    logger.warning(f"ê¸°ì‚¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    item["summary"] = text[:1000]
                    item["source"] = "Unknown"
                    item["summary_short"] = item.get("desc", "")
                    item["analysis"] = {"facts": [], "opinions": []}

                item["content"] = text
                item["images"] = final_images
                item["charts"] = charts
                return item
                
        except Exception as e:
            logger.error(f"Crawl failed {item['url']}: {e}")
            return None

    # ë³‘ë ¬ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_one, item): item for item in articles}
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: results.append(res)
            
    return results


def _structure_facts(articles: List[Dict]) -> List[Dict]:
    """
    [Fact Extractor]
    ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ ë¬¸ë‹¨(summary)ì„ ì¢…í•© ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ íŒ©íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì¤‘ë³µ ì œê±°, ì¶œì²˜ ë§¤í•‘, ì‹œê°í™” ì œì•ˆ(Visual Plan)ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    if not articles:
        return []
        
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key: return []
        
        # 1. ê¸°ì‚¬ í…ìŠ¤íŠ¸ ëª¨ìœ¼ê¸°
        combined_text = ""
        for i, art in enumerate(articles):
            summary = art.get("summary", "")
            if len(summary) > 50: # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ
                combined_text += f"\n[Article {i}] Title: {art['title']}\n{summary}\n"
        
        if len(combined_text) < 100:
            return []

        # 2. GPT ë¡œì§
        llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key, temperature=0.3)
        
        prompt = f"""
        Analyze the following news articles and extract structured facts.
        
        [TASK]
        1. **Extraction**: Extract key facts (Statistics, Quotes, Key Events).
        2. **Deduplication**: Merge similar facts from different articles into one.
        3. **Source Mapping**: Usage 'source_indices' list (e.g., [0, 2]) to track where the fact came from.
        4. **Visual Proposal**: Suggest how to visualize this fact in a video.
           - Options: "Chart" (for trends/comparisons), "Quote Card" (for sayings), "Text Overlay" (for key terms), "Timeline" (for dates), "None".
        
        [OUTPUT FORMAT]
        Return a JSON list of objects:
        [
          {{
            "category": "Statistic" | "Quote" | "Event" | "Fact",
            "content": "Fact description (e.g., Sales increased by 50%)",
            "value": "Key number/date if applicable (e.g., 50%)",
            "source_indices": [0, 2],
            "visual_proposal": "Chart" | "Quote Card" | "Text Overlay" | "Timeline" | "None"
          }}
        ]
        
        [ARTICLES]
        {combined_text[:10000]} 
        """
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ë³´í˜¸)
        
        msg = HumanMessage(content=prompt)
        res = llm.invoke([msg])
        
        # JSON íŒŒì‹±
        content = res.content.replace("```json", "").replace("```", "").strip()
        
        # ê°€ë” ë°°ì—´ì´ ì•„ë‹Œ ê°ì²´ë¡œ ì˜¤ê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ì„ì¼ ìˆ˜ ìˆìŒ -> íŒŒì‹± ì‹œë„
        try:
            facts = json.loads(content)
            if isinstance(facts, list):
                # [FIX] ê° íŒ©íŠ¸ì— UUID ë¶€ì—¬ ë° Article ID ë§¤í•‘ (Verifier ì—°ê²°ìš©)
                for fact in facts:
                    if "id" not in fact:
                        fact["id"] = f"fact-{uuid.uuid4().hex[:12]}"
                    
                    # Source Indices([0, 2])ë¥¼ -> ì‹¤ì œ article_idë¡œ ë³€í™˜
                    indices = fact.get("source_indices", [])
                    if indices and isinstance(indices, list):
                        # ì²« ë²ˆì§¸ ì¶œì²˜ë¥¼ ëŒ€í‘œ IDë¡œ ë§¤í•‘
                        first_idx = indices[0]
                        if isinstance(first_idx, int) and 0 <= first_idx < len(articles):
                            fact["article_id"] = articles[first_idx].get("id")

                return facts
            else:
                return []
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëŒ€ê´„í˜¸ ì°¾ì•„ì„œ ì¬ì‹œë„
            try:
                start = content.find('[')
                end = content.rfind(']')
                if start != -1 and end != -1:
                    parsed_facts = json.loads(content[start:end+1])
                    # UUID ë¶€ì—¬
                    for fact in parsed_facts:
                        if "id" not in fact:
                            fact["id"] = f"fact-{uuid.uuid4().hex[:12]}"
                    return parsed_facts
            except:
                pass
                
            return []
            
    except Exception as e:
        logger.warning(f"Fact Structure Failed: {e}")
        return []
