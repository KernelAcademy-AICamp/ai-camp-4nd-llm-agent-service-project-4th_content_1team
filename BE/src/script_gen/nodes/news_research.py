"""
News Research Node (Advanced) - ê³ ì„±ëŠ¥ ë‰´ìŠ¤ ë° ì‹œê° ìë£Œ ìˆ˜ì§‘ê¸°

[í•µì‹¬ ë¡œì§]
1. Deep Fetch: ê²€ìƒ‰ì–´ë‹¹ 15ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í›„ë³´êµ°ì„ ë„“í˜
2. Smart Dedup: ìœ ì‚¬í•œ ì£¼ì œì˜ ê¸°ì‚¬ë¥¼ ê·¸ë£¹í•‘í•˜ê³ , ê° ê·¸ë£¹ì—ì„œ 'ì•Œì§œ ê¸°ì‚¬' 1ê°œì”©ë§Œ ì„ ë³„ (Top 3)
3. Aggressive Crawl: í˜ì´ì§€ ë‚´ 100px ì´ìƒ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘ (ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ ìµœì†Œí™”)
4. AI Context Check: GPT-4o-miniê°€ ê¸°ì‚¬ ë³¸ë¬¸ ìš”ì•½ê³¼ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì§„ì§œ ì°¨íŠ¸/í‘œ ë°œêµ´
"""
from typing import Dict, Any, List, Optional
import requests
import json
import trafilatura
import logging
import concurrent.futures
import os
import hashlib
import re
import difflib  # ìœ ì‚¬ë„ ë¹„êµìš©
from datetime import datetime
from playwright.sync_api import sync_playwright
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

# ì„¤ì •
CRAWL_TIMEOUT = 40
MAX_WORKERS = 3 
SIMILARITY_THRESHOLD = 0.6  # ì œëª© ìœ ì‚¬ë„ ê¸°ì¤€ (0.6 ì´ìƒì´ë©´ ê°™ì€ ë‚´ìš©ìœ¼ë¡œ ê°„ì£¼)


def news_research_node(state: Dict[str, Any]) -> Dict[str, Any]:
    logger.info("News Research Node (Advanced) ì‹œì‘")
    
    # 1. ì…ë ¥ ë°ì´í„°
    content_brief = state.get("content_brief", {})
    research_plan = content_brief.get("researchPlan", {})
    base_queries = research_plan.get("newsQuery", [])
    
    if not base_queries:
        return {"news_data": {"articles": []}}
    
    # 2. ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ (Deep Fetch)
    # ì¿¼ë¦¬ë‹¹ 15ê°œì”© ìˆ˜ì§‘ -> í›„ë³´êµ° í™•ë³´
    logger.info(f"ë‰´ìŠ¤ í›„ë³´êµ° ìˆ˜ì§‘ ì‹œì‘: ì¿¼ë¦¬ë‹¹ 15ê°œ")
    raw_articles = _fetch_naver_news_bulk(base_queries)
    logger.info(f"ë‰´ìŠ¤ í›„ë³´êµ° í™•ë³´: {len(raw_articles)}ê°œ")
    
    # 3. ì¤‘ë³µ ì œê±° ë° ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • (Smart Dedup)
    # ë¹„ìŠ·í•œ ê¸°ì‚¬ëŠ” ë¬¶ì–´ì„œ ë²„ë¦¬ê³ , ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œì˜ ì•Œì§œ ê¸°ì‚¬ë§Œ ë‚¨ê¹€
    unique_articles = _deduplicate_articles(raw_articles)
    logger.info(f"ì¤‘ë³µ ì œê±° í›„ ì„ ë³„ëœ Top ê¸°ì‚¬: {len(unique_articles)}ê°œ")
    
    # 4. ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì •ë°€ í¬ë¡¤ë§ (Crawling & AI Analysis)
    # ì„ ë³„ëœ Top ê¸°ì‚¬ë“¤ì— ëŒ€í•´ì„œë§Œ ì •ë°€ ë¶„ì„ ìˆ˜í–‰ (ë¹„ìš© ì ˆê°)
    full_articles = _crawl_and_analyze(unique_articles)
    
    # 5. ê²°ê³¼ ë°˜í™˜ (ì°¨íŠ¸ê°€ ìˆëŠ” ê¸°ì‚¬ ìš°ì„  ì •ë ¬)
    full_articles.sort(key=lambda x: (len(x.get("charts", [])), len(x.get("images", []))), reverse=True)
    
    return {
        "news_data": {
            "articles": full_articles,
            "queries_used": base_queries,
            "collected_at": datetime.now().isoformat()
        }
    }


def _fetch_naver_news_bulk(queries: List[str]) -> List[Dict]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ê²°ê³¼ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì¿¼ë¦¬ë‹¹ 15ê°œ)."""
    articles = []
    seen_links = set()
    
    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")
    
    if not client_id or not client_secret:
        logger.error("NAVER API Key Missing")
        return []

    headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
    url = "https://openapi.naver.com/v1/search/news.json"
    
    for query in queries:
        try:
            # 15ê°œ ìˆ˜ì§‘ (display=15)
            params = {"query": query, "display": 15, "sort": "sim"}
            res = requests.get(url, headers=headers, params=params, timeout=5)
            
            if res.status_code == 200:
                items = res.json().get("items", [])
                for item in items:
                    link = item.get("originallink") or item.get("link")
                    if link and link not in seen_links:
                        clean_title = re.sub('<[^<]+?>', '', item.get("title", ""))
                        clean_desc = re.sub('<[^<]+?>', '', item.get("description", ""))
                        
                        articles.append({
                            "title": clean_title,
                            "url": link,
                            "desc": clean_desc,
                            "pub_date": item.get("pubDate"),
                            "query": query
                        })
                        seen_links.add(link)
        except Exception as e:
            logger.warning(f"Naver Search Error ({query}): {e}")
            
    return articles


def _deduplicate_articles(articles: List[Dict]) -> List[Dict]:
    """
    ìœ ì‚¬í•œ ì œëª©ì„ ê°€ì§„ ê¸°ì‚¬ë“¤ì„ ê·¸ë£¹í™”í•˜ê³ , 
    ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ì˜ì–‘ê°€ ìˆëŠ”(ì„¤ëª…ì´ ê¸¸ê±°ë‚˜ í‚¤ì›Œë“œê°€ ìˆëŠ”) ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì”© ë½‘ì•„ëƒ…ë‹ˆë‹¤.
    ìµœì¢…ì ìœ¼ë¡œ Top 3~5ê°œë§Œ ë¦¬í„´í•©ë‹ˆë‹¤.
    """
    if not articles:
        return []
        
    clusters = []
    visited = [False] * len(articles)
    
    # 1. Clustering (ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í•‘)
    for i in range(len(articles)):
        if visited[i]:
            continue
            
        current_cluster = [articles[i]]
        visited[i] = True
        
        for j in range(i + 1, len(articles)):
            if visited[j]:
                continue
                
            # ë¬¸ì¥ ìœ ì‚¬ë„ ë¹„êµ (ì œëª© OR ë‚´ìš© OR í‚¤ì›Œë“œ) - ì…‹ ì¤‘ í•˜ë‚˜ë¼ë„ ë†’ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            title_sim = difflib.SequenceMatcher(None, articles[i]["title"], articles[j]["title"]).ratio()
            desc_sim = difflib.SequenceMatcher(None, articles[i]["desc"], articles[j]["desc"]).ratio()
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ (Jaccard Similarity)
            # ì œëª©ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•˜ì—¬ êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
            words_i = set(articles[i]["title"].split())
            words_j = set(articles[j]["title"].split())
            intersection = len(words_i & words_j)
            union = len(words_i | words_j)
            keyword_sim = intersection / union if union > 0 else 0
            
            # ì œëª© 40% ì´ìƒ OR ë‚´ìš© 70% ì´ìƒ OR í‚¤ì›Œë“œ 50% ì´ìƒ ë¹„ìŠ·í•˜ë©´ ê°™ì€ ê¸°ì‚¬
            if title_sim >= 0.4 or desc_sim >= 0.7 or keyword_sim >= 0.5:
                current_cluster.append(articles[j])
                visited[j] = True
        
        clusters.append(current_cluster)
    
    # 2. Representative Selection (ëŒ€í‘œ ê¸°ì‚¬ ì„ ì •)
    final_articles = []
    for cluster in clusters:
        # ì ìˆ˜ ê³„ì‚°: ì„¤ëª… ê¸¸ì´ + ('í‘œ'/'ê·¸ë˜í”„' í‚¤ì›Œë“œ ê°€ì‚°ì )
        best_article = cluster[0]
        max_score = -1
        
        for art in cluster:
            score = len(art["desc"])  # ê¸°ë³¸ ì ìˆ˜: ì„¤ëª…ì´ ìì„¸í• ìˆ˜ë¡ ì¢‹ìŒ
            
            # ê°€ì‚°ì : í‘œ/ê·¸ë˜í”„/ì¢…í•©/ë¶„ì„ ê°™ì€ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ë°ì´í„°ê°€ ë§ì„ í™•ë¥  ë†’ìŒ
            keywords = ["í‘œ", "ê·¸ë˜í”„", "ì°¨íŠ¸", "ì¶”ì´", "í˜„í™©", "ì¢…í•©", "ë¶„ì„"]
            if any(k in art["title"] for k in keywords):
                score += 200
            if any(k in art["desc"] for k in keywords):
                score += 100
            
            if score > max_score:
                max_score = score
                best_article = art
                
        final_articles.append(best_article)
        
    # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë°˜í™˜ (ë‹¤ì–‘ì„± í™•ë³´ëœ ìƒíƒœ)
    return final_articles[:5]



import base64
import ssl
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager

class LegacySSLAdapter(HTTPAdapter):
    """ì˜¤ë˜ëœ ì„œë²„(SSL Legacy Renegotiation) ì ‘ì†ì„ ìœ„í•œ ì–´ëŒ‘í„°"""
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context()
        ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

import uuid

def download_image_to_local(image_url: str, referrer_url: str = None) -> Optional[str]:
    """
    ì´ë¯¸ì§€ë¥¼ public/images/news ê²½ë¡œì— ì €ì¥í•˜ê³  ìƒëŒ€ ê²½ë¡œë¥¼ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.
    """
    try:
        save_dir = "public/images/news"
        os.makedirs(save_dir, exist_ok=True)

        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": referrer_url if referrer_url else ""
        }
        
        res = session.get(image_url, headers=headers, timeout=10)
        if res.status_code != 200:
            return None
            
        ext = "jpg"
        if "png" in image_url.lower(): ext = "png"
        if "gif" in image_url.lower(): ext = "gif"
        
        filename = f"{uuid.uuid4()}.{ext}"
        filepath = os.path.join(save_dir, filename)
        
        with open(filepath, "wb") as f:
            f.write(res.content)
            
        return f"/images/news/{filename}"
    except Exception as e:
        logger.error(f"Image Download Failed: {e}")
        return None

import time

def _check_image_context(image_url: str, article_title: str, article_summary: str, referrer_url: str = None) -> Dict:
    """
    GPT-4o-miniì—ê²Œ [ê¸°ì‚¬ ìš”ì•½ + ì´ë¯¸ì§€(Base64)]ë¥¼ ë³´ì—¬ì£¼ê³  íŒë‹¨í•˜ê²Œ í•¨
    (Legacy SSL ì„œë²„ ì§€ì› + Rate Limit ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
    """
    max_retries = 3
    retry_delay = 2  # ì´ˆ
    
    for attempt in range(max_retries):
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key: return {"relevant": False}
            
            # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (Legacy SSL ì§€ì› Session ì‚¬ìš©)
            session = requests.Session()
            session.mount('https://', LegacySSLAdapter())
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Referer": referrer_url if referrer_url else ""
            }
            
            img_res = session.get(image_url, headers=headers, timeout=10)
            if img_res.status_code != 200:
                return {"relevant": False}
                
            # 2. Base64 ì¸ì½”ë”©
            encoded_string = base64.b64encode(img_res.content).decode("utf-8")
            data_url = f"data:image/jpeg;base64,{encoded_string}"
            
            llm = ChatOpenAI(model="gpt-4o-mini", api_key=api_key)
            
            prompt = f"""
            [ë¶„ì„ ìš”ì²­]
            ê¸°ì‚¬ ì œëª©: {article_title}
            ê¸°ì‚¬ ìš”ì•½: {article_summary}
            
            ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì„œ JSONìœ¼ë¡œ ë‹µí•´ì¤˜:
            1. relevant: ì´ ì´ë¯¸ì§€ê°€ ê¸°ì‚¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?
            2. type: "chart", "table", "photo", "other"
            3. description: ì´ë¯¸ì§€ ì„¤ëª… (í•œê¸€)
            
            íŒë‹¨ ê¸°ì¤€:
            - chart/table: ê¸°ì‚¬ì˜ ë°ì´í„°/í†µê³„ë¥¼ ì‹œê°í™”í•œ ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ
            - photo: ê¸°ì‚¬ ì£¼ì œì™€ ì§ì ‘ ì—°ê´€ëœ ì‚¬ì§„
              ì˜ˆ) ë¶€ë™ì‚° ê¸°ì‚¬ â†’ ì•„íŒŒíŠ¸/ê±´ë¬¼ ì‚¬ì§„ OK
              ì˜ˆ) ìŠ¤í¬ì¸  ê¸°ì‚¬ â†’ ì„ ìˆ˜/ê²½ê¸° ì‚¬ì§„ OK
              ì˜ˆ) ê²½ì œ ê¸°ì‚¬ â†’ ê´€ë ¨ í˜„ì¥/ì¸ë¬¼ ì‚¬ì§„ OK
            - other: ê´‘ê³ , ë¡œê³ , ë°°ë„ˆ, ì•„ì´ì½˜ â†’ relevant=false
            
            relevant=true ì¡°ê±´:
            - ì°¨íŠ¸/í‘œëŠ” ë¬´ì¡°ê±´ í¬í•¨
            - ì‚¬ì§„ì€ ê¸°ì‚¬ ì£¼ì œì™€ ëª…í™•íˆ ì—°ê´€ëœ ê²½ìš°ë§Œ í¬í•¨
            - ê´‘ê³ /ë¡œê³ /ë°°ë„ˆ/ê¸°ì í”„ë¡œí•„ ì‚¬ì§„ì€ ì œì™¸
            """
            
            msg = HumanMessage(content=[
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_url}}
            ])
            
            res = llm.invoke([msg])
            content = res.content.replace("```json", "").replace("```", "").strip()
            return json.loads(content)
            
        except Exception as e:
            if "rate_limit" in str(e).lower() or "429" in str(e):
                if attempt < max_retries - 1:
                    logger.warning(f"Rate limit hit, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    continue
            logger.warning(f"AI Check Error: {e}")
            return {"relevant": False}
    return {"relevant": False}


def _crawl_and_analyze(articles: List[Dict]) -> List[Dict]:
    """Playwrightë¡œ ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ ë° ì´ë¯¸ì§€ë¥¼ ì‹¹ ê¸ì–´ì˜¤ê³  AIë¡œ ë¶„ì„"""
    results = []
    
    if not articles:
        return []

    def process_one(item):
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                
                # ğŸ­ ë´‡ íƒì§€ ìš°íšŒ: User-Agent ë³€ê²½
                page = browser.new_page(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
                
                # ë¡œë”© ëŒ€ê¸°
                try:
                    page.goto(item["url"], timeout=CRAWL_TIMEOUT*1000, wait_until="domcontentloaded")
                    
                    # [Scroll Logic] Lazy Loading ì´ë¯¸ì§€ ë¡œë”©ì„ ìœ„í•´ ìŠ¤í¬ë¡¤ ë‹¤ìš´
                    for _ in range(5):
                        page.evaluate("window.scrollBy(0, document.body.scrollHeight / 5)")
                        page.wait_for_timeout(2000)  # 2.0ì´ˆ ëŒ€ê¸° (Wait longer for lazy loading)
                        
                except:
                    browser.close()
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_html = page.content()
                text = trafilatura.extract(content_html, include_links=False)
                if not text or len(text) < 50:
                    browser.close()
                    return None
                
                # ì´ë¯¸ì§€ ì¶”ì¶œ (Lazy Loading ì§€ì› + Aggressive Mode)
                # data-src, data-original, data-url ìš°ì„  í™•ì¸
                images_found = []
                
                # [ê°œì„ ] ìŠ¤ë§ˆíŠ¸ ë³¸ë¬¸ ì˜ì—­ ê°ì§€ ì•Œê³ ë¦¬ì¦˜
                def find_article_body_smart(page):
                    """
                    íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ì„ ìë™ ê°ì§€
                    - í…ìŠ¤íŠ¸ ë°€ë„, ë¬¸ë‹¨ ê°œìˆ˜, ì´ë¯¸ì§€ ë“±ì„ ì¢…í•© í‰ê°€
                    """
                    # 1ë‹¨ê³„: ê¸°ì¡´ ì„ íƒì ì‹œë„ (ë¹ ë¥¸ ê²½ë¡œ)
                    common_selectors = [
                        "article", 
                        ".article-body", ".article_body", 
                        "#articleBody", "#newsBody",
                        "div[itemprop='articleBody']",
                    ]
                    
                    for selector in common_selectors:
                        try:
                            elem = page.query_selector(selector)
                            if elem and len(elem.inner_text()) > 200:
                                logger.debug(f"[ARTICLE] ê¸°ì¡´ ì„ íƒìë¡œ ë³¸ë¬¸ ë°œê²¬: {selector}")
                                return elem
                        except:
                            continue
                    
                    # 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ - í›„ë³´ ìš”ì†Œ ìˆ˜ì§‘
                    logger.debug("[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ íƒìƒ‰ ì‹œì‘")
                    candidates = page.query_selector_all('div, section, article')
                    
                    best_score = -999999
                    best_elem = None
                    
                    for elem in candidates:
                        try:
                            text = elem.inner_text()
                            text_len = len(text)
                            
                            # ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                            if text_len < 200:
                                continue
                            
                            # ì ìˆ˜ ê³„ì‚° ìš”ì†Œ
                            p_count = len(elem.query_selector_all('p'))
                            li_count = len(elem.query_selector_all('li'))
                            img_count = len(elem.query_selector_all('img'))
                            link_count = len(elem.query_selector_all('a'))
                            
                            # ì ìˆ˜ = í…ìŠ¤íŠ¸ ê¸¸ì´ + ë¬¸ë‹¨ + ì´ë¯¸ì§€ - ë§í¬
                            score = (text_len * 0.3) + ((p_count + li_count) * 100) + (img_count * 50) - (link_count * 10)
                            
                            logger.debug(f"[ARTICLE] í›„ë³´: text={text_len}, p={p_count}, img={img_count}, link={link_count}, score={score:.0f}")
                            
                            if score > best_score:
                                best_score = score
                                best_elem = elem
                        except:
                            continue
                    
                    if best_elem:
                        logger.info(f"[ARTICLE] ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ë°œê²¬ (ì ìˆ˜: {best_score:.0f})")
                        return best_elem
                    
                    # 3ë‹¨ê³„: í´ë°± - body ì „ì²´
                    logger.warning("[ARTICLE] ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨. body ì „ì²´ ì‚¬ìš©")
                    return page.query_selector('body')
                
                # ìŠ¤ë§ˆíŠ¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
                article_area = find_article_body_smart(page)
                
                # ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì•˜ìœ¼ë©´ ê·¸ ì•ˆì—ì„œë§Œ, ëª» ì°¾ì•˜ìœ¼ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰
                search_area = article_area if article_area else page
                
                # 1. img íƒœê·¸
                imgs = search_area.query_selector_all("img")
                logger.debug(f"[ARTICLE] ê²€ìƒ‰ ì˜ì—­ì—ì„œ ë°œê²¬í•œ img íƒœê·¸: {len(imgs)}ê°œ")
                
                for img in imgs:
                    # Lazy Loading ì†ì„± í™•ì¸
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                        # [Filter] ì“°ë ˆê¸° ì´ë¯¸ì§€ ì œê±° (ê¸°ì, ì•„ì´ì½˜, ë°°ë„ˆ ë“±)
                        src_lower = src.lower()
                        trash_keywords = [
                            '.svg', '.gif', 'logo', 'icon', 'banner', 'ad', 'button', 'btn',
                            'reporter', 'profile', 'journalist', 'avatar'  # ê¸°ì/í”„ë¡œí•„ ì‚¬ì§„ í•„í„° ì¶”ê°€
                        ]
                        if any(x in src_lower for x in trash_keywords):
                            logger.debug(f"[FILTER] í‚¤ì›Œë“œ ì°¨ë‹¨: {src[:80]}")
                            continue
                            
                        # í¬ê¸° ì²´í¬ (JS)
                        try:
                            w = img.evaluate("el => el.naturalWidth")
                            h = img.evaluate("el => el.naturalHeight")
                            
                            # [Filter] í¬ê¸° ê¸°ì¤€ ìƒí–¥ (50px -> 150px)
                            # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì •ë³´ê°€ì¹˜ê°€ ì—†ìŒ
                            if w > 0 and w < 150 and h > 0 and h < 150:
                                logger.debug(f"[FILTER] í¬ê¸° ì°¨ë‹¨: {src[:80]} ({w}x{h})")
                                continue
                                
                            # Lazy Loading ì´ˆê¸°í™” ì „ì´ë¼ 0ì¼ ìˆ˜ë„ ìˆìŒ -> ì¼ë‹¨ URL ë¯¿ê³  ìˆ˜ì§‘ (AIê°€ ìµœì¢… íŒë³„)
                            images_found.append({"url": src, "width": w, "height": h})
                        except:
                            # í¬ê¸° í™•ì¸ ì‹¤íŒ¨í•´ë„ URLì´ ì •ìƒì´ë©´ ì¼ë‹¨ ì¶”ê°€
                            images_found.append({"url": src, "width": 0, "height": 0})
                            
                # 2. figure ì•ˆì˜ ì´ë¯¸ì§€ (ë³´í†µ ì¤‘ìš”í•œ ê¸°ì‚¬ ì´ë¯¸ì§€)
                figures = search_area.query_selector_all("figure img")
                for img in figures:
                    src = None
                    for attr in ["data-src", "data-original", "data-url", "src"]:
                        val = img.get_attribute(attr)
                        if val and val.startswith("http"):
                            src = val
                            break
                    
                    if src:
                         images_found.append({"url": src, "width": 0, "height": 0})

                # [DEBUG] ì´ë¯¸ì§€ ë°œê²¬ ì§í›„ ë¡œê¹…
                logger.info(f"[DEBUG] {item['url']} - ì›ë³¸ ì´ë¯¸ì§€ ë°œê²¬: {len(images_found)}ê°œ")
                for idx, img in enumerate(images_found[:10]):
                    logger.info(f"  [{idx+1}] {img['url'][:100]}... (w:{img['width']}, h:{img['height']})")
                
                # ì¤‘ë³µ URL ì œê±°
                seen_urls = set()
                candidates = []
                for img in images_found:
                    if img["url"] not in seen_urls:
                        candidates.append(img)
                        seen_urls.add(img["url"])
                
                # [DEBUG] ì¤‘ë³µ ì œê±° í›„ ë¡œê¹…
                logger.info(f"[DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(candidates)}ê°œ")
                
                # --- AI ë¶„ì„ ë‹¨ê³„ (Context check) ---
                final_images = []
                charts = []
                
                # ê¸°ì‚¬ ìš”ì•½ (ì•ë¶€ë¶„ 500ì) - AIì—ê²Œ ë¬¸ë§¥ ì œê³µìš©
                summary = text[:500]
                
                # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ AI ê²€ìˆ˜ (ë¹„ìš© ì¡°ì ˆ) [Change] 8 -> 5
                logger.info(f"[DEBUG] AI ë¶„ì„ ì‹œì‘: {len(candidates[:5])}ê°œ ì´ë¯¸ì§€")
                for img in candidates[:5]:
                    # Refererë¡œ ê¸°ì‚¬ URL ì „ë‹¬í•˜ì—¬ Hotlink Protection ìš°íšŒ
                    analysis = _check_image_context(img["url"], item["title"], summary, referrer_url=item["url"])
                    
                    # [DEBUG] AI ë¶„ì„ ê²°ê³¼ ë¡œê¹…
                    if not analysis.get("relevant"):
                        logger.debug(f"[AI] ê±°ë¶€ë¨: {img['url'][:80]} - {analysis}")
                    
                    if analysis.get("relevant"):
                        img_data = {
                            "url": img["url"],
                            "width": img.get("width", 0),
                            "height": img.get("height", 0),
                            "type": analysis.get("type", "other"),
                            "desc": analysis.get("description", "")
                        }
                        
                        # [Local Save] ì´ë¯¸ì§€ ë¡œì»¬ ì €ì¥
                        local_path = download_image_to_local(img["url"], item["url"])
                        if local_path:
                            img_data["url"] = local_path
                        else:
                            # ì €ì¥ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ìœ ì§€ (í˜¹ì€ ì œì™¸ ì •ì±…ì— ë”°ë¼ continue ê°€ëŠ¥)
                            pass
                        
                        # [ë¶„ë¦¬ ë¡œì§] ì°¨íŠ¸/í‘œëŠ” chartsì—ë§Œ, ë‚˜ë¨¸ì§€ëŠ” imagesì—ë§Œ ë„£ê¸°
                        if analysis.get("type") in ["chart", "table"]:
                            charts.append(img_data)
                        else:
                            final_images.append(img_data)
                
                browser.close()
                
                item["content"] = text
                item["images"] = final_images
                item["charts"] = charts
                return item
                
        except Exception as e:
            logger.error(f"Crawl failed {item['url']}: {e}")
            return None

    # ë³‘ë ¬ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_one, item): item for item in articles}
        for f in concurrent.futures.as_completed(futures):
            res = f.result()
            if res: results.append(res)
            
    return results
